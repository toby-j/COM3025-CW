{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgNdhpLOvFjm"
   },
   "source": [
    "# Imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "qDkEGwmWu8Bx",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.119677700Z",
     "start_time": "2023-05-13T11:26:33.840438300Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from keras import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import PIL.Image\n",
    "from torchvision import models, transforms\n",
    "from matplotlib import pylab as P\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7A6B6RebmmE"
   },
   "source": [
    "\n",
    "# Global functions"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def plot_confusion_matrix(model_input, test_data, test_labels):\n",
    "  predictions = model_input.predict(test_data)\n",
    "\n",
    "  y_pred  = np.argmax(predictions, axis=-1)\n",
    "  y_test = np.argmax(test_labels, axis=-1)\n",
    "\n",
    "  confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "  labels = list(set(y_test) | set(y_pred))\n",
    "  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=labels)\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(10, 8))\n",
    "  cm_display.plot(ax=ax)\n",
    "\n",
    "\n",
    "  plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.154694900Z",
     "start_time": "2023-05-13T11:26:40.123676300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7jxu5s9dbp9P",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.175693500Z",
     "start_time": "2023-05-13T11:26:40.139694100Z"
    }
   },
   "outputs": [],
   "source": [
    "def output_metrics(model, X_test, y_test):\n",
    "    # Use the trained model to make predictions on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = precision_score(y_test, y_pred_classes, average='weighted')\n",
    "\n",
    "    # Calculate recall\n",
    "    recall = recall_score(y_test, y_pred_classes, average='weighted')\n",
    "\n",
    "    # Print the metrics\n",
    "    print(f'F1 score: {f1:.2f}')\n",
    "    print(f'Precision: {precision:.2f}')\n",
    "    print(f'Recall: {recall:.2f}')\n",
    "    print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder_path):\n",
    "    image_paths = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            image_paths.append(img_path)\n",
    "    return image_paths"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.176728100Z",
     "start_time": "2023-05-13T11:26:40.155692700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def read_and_resize_image(image_path, size):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, size)\n",
    "    return img"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.203697200Z",
     "start_time": "2023-05-13T11:26:40.168692900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def normaliseGradients(image_3d, percentile=99):\n",
    "    image_2d = np.sum(np.abs(image_3d), axis=2)\n",
    "\n",
    "    # Get max pixel value in the image\n",
    "    vmax = np.percentile(image_2d, percentile)\n",
    "    # Get minimum pixel value in the image\n",
    "    vmin = np.min(image_2d)\n",
    "\n",
    "    # Normalise the values. We clip intensities so values lower than 0 are equal 0.\n",
    "    return np.clip((image_2d - vmin) / (vmax - vmin), 0, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.253719400Z",
     "start_time": "2023-05-13T11:26:40.186692800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def convert_to_boolean_mask(image):\n",
    "    # Convert the image to a NumPy array\n",
    "    image = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Create a boolean mask where white pixels are True and black pixels are False\n",
    "    binary_image = np.where(image == 255, False, True)  # Assuming white pixels are represented as 255\n",
    "\n",
    "    cropped_image = cv2.resize(binary_image.astype(np.uint8), (64, 64))\n",
    "\n",
    "    return cropped_image"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.253719400Z",
     "start_time": "2023-05-13T11:26:40.201698Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "def calculate_overlap(g, m):\n",
    "    # Invert the mask, so the pixels outside are True.\n",
    "    # Replace where the mask is False, with a 0 in the same location in raw_gradients\n",
    "    segment = np.where(np.array(m), np.array(g), 0)\n",
    "\n",
    "    # We now have just the gradients in a 2D vector of the pixels outside the bounding box\n",
    "    sum_mask_segment = np.sum(segment)\n",
    "    # Find what percentage the outside pixels make up of the full gradient image by summing both 2D vectors\n",
    "    total_sum = np.sum(g)\n",
    "    # What percentage are the gradients outside the segment of the full gradient vector\n",
    "    overlap = (sum_mask_segment / total_sum) * 100\n",
    "\n",
    "    return overlap"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T12:32:24.830687900Z",
     "start_time": "2023-05-13T12:32:24.775654200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utilities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "fGgdn5RUfZ-V",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:38:31.397293800Z",
     "start_time": "2023-05-13T11:38:31.383282900Z"
    }
   },
   "outputs": [],
   "source": [
    "def ShowGrayscaleImage(im, title='', ax=None):\n",
    "  if ax is None:\n",
    "    P.figure()\n",
    "  P.axis('off')\n",
    "\n",
    "  P.imshow(im, cmap=P.cm.gray, vmin=0, vmax=1)\n",
    "  P.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "transformer = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "def PreprocessImages(images):\n",
    "    # assumes input is 4-D, with range [0,255]\n",
    "    #\n",
    "    # torchvision have color channel as first dimension\n",
    "    # with normalization relative to mean/std of ImageNet:\n",
    "    #    https://pytorch.org/vision/stable/models.html\n",
    "    images = np.array(images)\n",
    "    images = images/255\n",
    "    images = np.transpose(images, (0,3,1,2))\n",
    "    images = torch.tensor(images, dtype=torch.float32)\n",
    "    images = transformer.forward(images)\n",
    "    return images.requires_grad_(True)"
   ],
   "metadata": {
    "id": "10o0c6Nxwvdn",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:38:31.417797500Z",
     "start_time": "2023-05-13T11:38:31.397293800Z"
    }
   },
   "execution_count": 70,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def visualiseImageToHeatmap(image_3d, percentile=99):\n",
    "    r\"\"\"Returns a 3D tensor as RGB 3D heatmap\n",
    "    Pixels with higher weightage in sailiency heatmap will most saturated and will correspond to high RGB values in output heatmap_rgb\n",
    "  \"\"\"\n",
    "    image_2d = normaliseGradients(image_3d)\n",
    "    # Create heatmap using \"jet\" colormap, which returns an RGBA image\n",
    "    heatmap = plt.get_cmap('jet')(image_2d) * 255\n",
    "\n",
    "    # Normalise to 0,255 so it's visible when pasted\n",
    "    return Image.fromarray(heatmap.astype(np.uint8), mode='RGBA'), image_2d"
   ],
   "metadata": {
    "id": "1Eifbs32w2CP",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:38:31.447795700Z",
     "start_time": "2023-05-13T11:38:31.413796800Z"
    }
   },
   "execution_count": 71,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def LoadImage(file_path):\n",
    "    im = PIL.Image.open(file_path)\n",
    "    im = im.resize((299, 299))\n",
    "    im = np.asarray(im)\n",
    "    return im"
   ],
   "metadata": {
    "id": "bH5P4ljFw-GB",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:38:31.447795700Z",
     "start_time": "2023-05-13T11:38:31.427797900Z"
    }
   },
   "execution_count": 72,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def ShowImage(im, title='', ax=None):\n",
    "    if ax is None:\n",
    "        P.figure()\n",
    "    P.axis('off')\n",
    "    P.imshow(im)\n",
    "    P.title(title)"
   ],
   "metadata": {
    "id": "YXpx3NuHxHU7",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:38:31.466769200Z",
     "start_time": "2023-05-13T11:38:31.444798200Z"
    }
   },
   "execution_count": 73,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Global Variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# This variable is used to globally set the size of the images used for training and viaulisation.\n",
    "# All images must be the same size for the techniques to work.\n",
    "image_size = (64, 64)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.340720400Z",
     "start_time": "2023-05-13T11:26:40.310698700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rwaor_SUveCb"
   },
   "source": [
    "# Kaggle dataset loading\n",
    "If you're using your local machine, download the dataset into root/contents\n",
    "Kaggle doesn't have the segmentation images. Download this folder from the Harvard dataverse and place in /contents:\n",
    "\n",
    "https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T\n",
    "https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000\n",
    "\n",
    "You need a kaggle.json api key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Qp8DNrdxvi97",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.340720400Z",
     "start_time": "2023-05-13T11:26:40.327692200Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -q kaggle\n",
    "# from google.colab import files\n",
    "# files.upload()\n",
    "# !mkdir ~/.kaggle\n",
    "# !cp kaggle.json ~/.kaggle/\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "# !kaggle datasets download -d kmader/skin-cancer-mnist-ham10000\n",
    "# !unzip -q skin-cancer-mnist-ham10000.zip -d content\n",
    "# Removing the zip to save space\n",
    "# !rm skin-cancer-mnist-ham10000.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ek5Dfin9Svmd"
   },
   "source": [
    "## Dataset cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GEzN-8pAu8Bz",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.426306900Z",
     "start_time": "2023-05-13T11:26:40.341701Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('content/HAM10000_metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "41stH92zu8B-",
    "outputId": "0bdefcbd-0b66-4268-972f-6708465ab91c",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.426306900Z",
     "start_time": "2023-05-13T11:26:40.374694500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "               age\ncount  9958.000000\nmean     51.863828\nstd      16.968614\nmin       0.000000\n25%      40.000000\n50%      50.000000\n75%      65.000000\nmax      85.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>9958.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>51.863828</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>16.968614</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>40.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>50.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>65.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>85.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "U6CC22ABu8CB",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.426306900Z",
     "start_time": "2023-05-13T11:26:40.407278800Z"
    }
   },
   "outputs": [],
   "source": [
    "lesion_type_dict = {\n",
    "    'nv': 'Melanocytic nevi',\n",
    "    'mel': 'Melanoma',\n",
    "    'bkl': 'Bening keratosis-like lesions',\n",
    "    'bcc': 'Basal cell carcinoma',\n",
    "    'akiec': 'Actinic keratoses',\n",
    "    'vasc': 'Vascular lesions',\n",
    "    'df': 'Dermatofibroma'\n",
    "}\n",
    "ds_dir = 'content/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "2-i_Kyq9u8CC",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.608819700Z",
     "start_time": "2023-05-13T11:26:40.423278500Z"
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "## Let's map the image_id with it's image path from part 1 and part 2 folders\n",
    "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x\n",
    "                     for x in glob(os.path.join(ds_dir, '*', '*.jpg'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Ge6Zi_Plu8CK",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.609805900Z",
     "start_time": "2023-05-13T11:26:40.553799Z"
    }
   },
   "outputs": [],
   "source": [
    "df['path'] = df['image_id'].map(imageid_path_dict.get)\n",
    "df['cell_type'] = df['dx'].map(lesion_type_dict.get)\n",
    "df['cell_type_idx'] = pd.Categorical(df['cell_type']).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0WLC4Dnu8CT",
    "outputId": "a4172b60-3976-445b-e697-758aa73cb0d3",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.609805900Z",
     "start_time": "2023-05-13T11:26:40.554800200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "lesion_id         0\nimage_id          0\ndx                0\ndx_type           0\nage              57\nsex               0\nlocalization      0\ndataset           0\npath              0\ncell_type         0\ncell_type_idx     0\ndtype: int64"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Fill in the null values with the average age\n",
    "df['age'].fillna((df['age'].mean()), inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.610827500Z",
     "start_time": "2023-05-13T11:26:40.567798900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "lesion_id        0\nimage_id         0\ndx               0\ndx_type          0\nage              0\nsex              0\nlocalization     0\ndataset          0\npath             0\ncell_type        0\ncell_type_idx    0\ndtype: int64"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check our dataset is cleaned for null values\n",
    "df.isna().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.610827500Z",
     "start_time": "2023-05-13T11:26:40.579805900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JElKkmMj_8j"
   },
   "source": [
    "# Smoothgrad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "wiuvG8mOsNqk",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.610827500Z",
     "start_time": "2023-05-13T11:26:40.595801700Z"
    }
   },
   "outputs": [],
   "source": [
    "class_idx_str = 'class_idx_str'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "N0FkwjCJs8_n",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.670089300Z",
     "start_time": "2023-05-13T11:26:40.612800700Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_OUTPUT_GRADIENTS = 'INPUT_OUTPUT_GRADIENTS'\n",
    "CONVOLUTION_LAYER_VALUES = 'CONVOLUTION_LAYER_VALUES'\n",
    "CONVOLUTION_OUTPUT_GRADIENTS = 'CONVOLUTION_OUTPUT_GRADIENTS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Sa6perPzzK4E",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.757088500Z",
     "start_time": "2023-05-13T11:26:40.629089900Z"
    }
   },
   "outputs": [],
   "source": [
    "expected_keys = [INPUT_OUTPUT_GRADIENTS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "xMKDQNRNsekV",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.757088500Z",
     "start_time": "2023-05-13T11:26:40.645089100Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_layer_outputs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "_XQ7SBxWqcSd",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.758087800Z",
     "start_time": "2023-05-13T11:26:40.666089900Z"
    }
   },
   "outputs": [],
   "source": [
    "def call_model_function(images, call_model_args=None, expected_keys=None):\n",
    "    target_class_idx =  call_model_args[class_idx_str]\n",
    "    images = tf.convert_to_tensor(images)\n",
    "    with tf.GradientTape() as tape:\n",
    "        if expected_keys==[INPUT_OUTPUT_GRADIENTS]:\n",
    "            tape.watch(images)\n",
    "            _, output_layer = new_model(images)\n",
    "            output_layer = output_layer[:,target_class_idx]\n",
    "            gradients = np.array(tape.gradient(output_layer, images))\n",
    "            return {INPUT_OUTPUT_GRADIENTS: gradients}\n",
    "        else:\n",
    "            conv_layer, output_layer = new_model(images)\n",
    "            gradients = np.array(tape.gradient(output_layer, conv_layer))\n",
    "            return {CONVOLUTION_LAYER_VALUES: conv_layer,\n",
    "                    CONVOLUTION_OUTPUT_GRADIENTS: gradients}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "6mbaFPIGOiYU",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.758087800Z",
     "start_time": "2023-05-13T11:26:40.696091100Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_and_check_call_model_output(self, output, input_shape, expected_keys):\n",
    "  \"\"\"Converts keys in the output into an np.ndarray, and confirms its shape.\n",
    "\n",
    "  Args:\n",
    "    output: The output dictionary of data to be formatted.\n",
    "    input_shape: The shape of the input that yielded the output\n",
    "    expected_keys: List of keys inside output to format/check for shape agreement.\n",
    "\n",
    "  Raises:\n",
    "      ValueError: If output shapes do not match expected shape.\"\"\"\n",
    "  # If key is in check_full_shape, the shape should be equal to the input shape (e.g. \n",
    "  # INPUT_OUTPUT_GRADIENTS, which gives gradients for each value of the input). Otherwise,\n",
    "  # only checks the outermost dimension of output to match input_shape (i.e. the batch size\n",
    "  # should be the same).\n",
    "  check_full_shape = [INPUT_OUTPUT_GRADIENTS]\n",
    "  for expected_key in expected_keys:\n",
    "    output[expected_key] = np.asarray(output[expected_key])\n",
    "    expected_shape = input_shape\n",
    "    actual_shape = output[expected_key].shape\n",
    "    if expected_key not in check_full_shape:\n",
    "      expected_shape = expected_shape[0]\n",
    "      actual_shape = actual_shape[0]\n",
    "    if expected_shape != actual_shape:\n",
    "      raise ValueError(SHAPE_ERROR_MESSAGE[expected_key].format(\n",
    "                      expected_shape, actual_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "xzMDaanyQOa4",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.759087600Z",
     "start_time": "2023-05-13T11:26:40.715092300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Output of the last convolution layer for the given input, including the batch\n",
    "# dimension.\n",
    "CONVOLUTION_LAYER_VALUES = 'CONVOLUTION_LAYER_VALUES'\n",
    "# Gradients of the output being explained (the logit/softmax value) with respect\n",
    "# to the last convolution layer, including the batch dimension.\n",
    "CONVOLUTION_OUTPUT_GRADIENTS = 'CONVOLUTION_OUTPUT_GRADIENTS'\n",
    "# Gradients of the output being explained (the logit/softmax value) with respect\n",
    "# to the input. Shape should be the same shape as x_value_batch.\n",
    "INPUT_OUTPUT_GRADIENTS = 'INPUT_OUTPUT_GRADIENTS'\n",
    "# Value of the output being explained (the logit/softmax value).\n",
    "OUTPUT_LAYER_VALUES = 'OUTPUT_LAYER_VALUES'\n",
    "\n",
    "SHAPE_ERROR_MESSAGE = {\n",
    "    CONVOLUTION_LAYER_VALUES: (\n",
    "        'Expected outermost dimension of CONVOLUTION_LAYER_VALUES to be the '\n",
    "        'same as x_value_batch - expected {}, actual {}'\n",
    "    ),\n",
    "    CONVOLUTION_OUTPUT_GRADIENTS: (\n",
    "        'Expected outermost dimension of CONVOLUTION_OUTPUT_GRADIENTS to be the '\n",
    "        'same as x_value_batch - expected {}, actual {}'\n",
    "    ),\n",
    "    INPUT_OUTPUT_GRADIENTS: (\n",
    "        'Expected key INPUT_OUTPUT_GRADIENTS to be the same shape as input '\n",
    "        'x_value_batch - expected {}, actual {}'\n",
    "    ),\n",
    "    OUTPUT_LAYER_VALUES: (\n",
    "        'Expected outermost dimension of OUTPUT_LAYER_VALUES to be the same as'\n",
    "        ' x_value_batch - expected {}, actual {}'\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "class CoreGradients(object):\n",
    "\n",
    "  def GetMask(self, x_value, call_model_function, call_model_args=None):\n",
    "    \"\"\"Returns an unsmoothed mask.\n",
    "\n",
    "    Args:\n",
    "      x_value: Input ndarray.\n",
    "      call_model_function: A function that interfaces with a model to return\n",
    "        specific output in a dictionary when given an input and other arguments.\n",
    "        Expected function signature:\n",
    "        - call_model_function(x_value_batch,\n",
    "                              call_model_args=None,\n",
    "                              expected_keys=None):\n",
    "          x_value_batch - Input for the model, given as a batch (i.e. dimension\n",
    "            0 is the batch dimension, dimensions 1 through n represent a single\n",
    "            input).\n",
    "          call_model_args - Other arguments used to call and run the model.\n",
    "          expected_keys - List of keys that are expected in the output. Possible\n",
    "            keys in this list are CONVOLUTION_LAYER_VALUES, \n",
    "            CONVOLUTION_OUTPUT_GRADIENTS, INPUT_OUTPUT_GRADIENTS, and\n",
    "            OUTPUT_LAYER_VALUES, and are explained in detail where declared.\n",
    "      call_model_args: The arguments that will be passed to the call model\n",
    "        function, for every call of the model.\n",
    "\n",
    "    \"\"\"\n",
    "    raise NotImplementedError('A derived class should implemented GetMask()')\n",
    "\n",
    "  def GetSmoothedMask(self,\n",
    "                      x_value,\n",
    "                      call_model_function,\n",
    "                      call_model_args=None,\n",
    "                      stdev_spread=.15,\n",
    "                      nsamples=25,\n",
    "                      magnitude=True,\n",
    "                      **kwargs):\n",
    "    \"\"\"Returns a mask that is smoothed with the SmoothGrad method.\n",
    "\n",
    "    Args:\n",
    "      x_value: Input ndarray.\n",
    "      call_model_function: A function that interfaces with a model to return\n",
    "        specific output in a dictionary when given an input and other arguments.\n",
    "        Expected function signature:\n",
    "        - call_model_function(x_value_batch,\n",
    "                              call_model_args=None,\n",
    "                              expected_keys=None):\n",
    "          x_value_batch - Input for the model, given as a batch (i.e. dimension\n",
    "            0 is the batch dimension, dimensions 1 through n represent a single\n",
    "            input).\n",
    "          call_model_args - Other arguments used to call and run the model.\n",
    "          expected_keys - List of keys that are expected in the output. Possible\n",
    "            keys in this list are CONVOLUTION_LAYER_VALUES,\n",
    "            CONVOLUTION_OUTPUT_GRADIENTS, INPUT_OUTPUT_GRADIENTS, and\n",
    "            OUTPUT_LAYER_VALUES, and are explained in detail where declared.\n",
    "      call_model_args: The arguments that will be passed to the call model\n",
    "        function, for every call of the model.\n",
    "      stdev_spread: Amount of noise to add to the input, as fraction of the\n",
    "                    total spread (x_max - x_min). Defaults to 15%.\n",
    "      nsamples: Number of samples to average across to get the smooth gradient.\n",
    "      magnitude: If true, computes the sum of squares of gradients instead of\n",
    "                 just the sum. Defaults to true.\n",
    "    \"\"\"\n",
    "    stdev = stdev_spread * (np.max(x_value) - np.min(x_value))\n",
    "    # Starting baseline image\n",
    "    total_gradients = np.zeros_like(x_value, dtype=np.float32)\n",
    "    for _ in range(nsamples):\n",
    "      noise = np.random.normal(0, stdev, x_value.shape)\n",
    "      # Calculate and add our smoothgrad noise\n",
    "      x_plus_noise = x_value + noise\n",
    "      # Get vanilla gradients. The input is the interpolated image + the Smoothgrad noise we generated\n",
    "      grad = self.GetMask(x_plus_noise, call_model_function, call_model_args,\n",
    "                          **kwargs)\n",
    "      if magnitude:\n",
    "        total_gradients += (grad * grad)\n",
    "      else:\n",
    "        total_gradients += grad\n",
    "\n",
    "    return total_gradients / nsamples\n",
    "\n",
    "  def format_and_check_call_model_output(self, output, input_shape, expected_keys):\n",
    "    \"\"\"Converts keys in the output into an np.ndarray, and confirms its shape.\n",
    "\n",
    "    Args:\n",
    "      output: The output dictionary of data to be formatted.\n",
    "      input_shape: The shape of the input that yielded the output\n",
    "      expected_keys: List of keys inside output to format/check for shape agreement.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If output shapes do not match expected shape.\"\"\"\n",
    "    # If key is in check_full_shape, the shape should be equal to the input shape (e.g. \n",
    "    # INPUT_OUTPUT_GRADIENTS, which gives gradients for each value of the input). Otherwise,\n",
    "    # only checks the outermost dimension of output to match input_shape (i.e. the batch size\n",
    "    # should be the same).\n",
    "    check_full_shape = [INPUT_OUTPUT_GRADIENTS]\n",
    "    for expected_key in expected_keys:\n",
    "      output[expected_key] = np.asarray(output[expected_key])\n",
    "      expected_shape = input_shape\n",
    "      actual_shape = output[expected_key].shape\n",
    "      if expected_key not in check_full_shape:\n",
    "        expected_shape = expected_shape[0]\n",
    "        actual_shape = actual_shape[0]\n",
    "      if expected_shape != actual_shape:\n",
    "        raise ValueError(SHAPE_ERROR_MESSAGE[expected_key].format(\n",
    "                       expected_shape, actual_shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "vMYQuC0onOFp",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:26:40.759087600Z",
     "start_time": "2023-05-13T11:26:40.741091100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inherits our smoothgrad technique in CoreGradients\n",
    "class Gradients(CoreGradients):\n",
    "\n",
    "  expected_keys = [INPUT_OUTPUT_GRADIENTS]\n",
    "\n",
    "  def GetMask(self, x_value, call_model_function, call_model_args=None):\n",
    "    \"\"\"Returns a vanilla gradients mask.\n",
    "\n",
    "    Args:\n",
    "      x_value: Input ndarray.\n",
    "      call_model_function: A function that interfaces with a model to return\n",
    "        specific data in a dictionary when given an input and other arguments.\n",
    "        Expected function signature:\n",
    "        - call_model_function(x_value_batch,\n",
    "                              call_model_args=None,\n",
    "                              expected_keys=None):\n",
    "          x_value_batch - Input for the model, given as a batch (i.e. dimension\n",
    "            0 is the batch dimension, dimensions 1 through n represent a single\n",
    "            input).\n",
    "          call_model_args - Other arguments used to call and run the model.\n",
    "          expected_keys - List of keys that are expected in the output. For this\n",
    "            method (Gradients), the expected keys are\n",
    "            INPUT_OUTPUT_GRADIENTS - Gradients of the output layer\n",
    "              (logit/softmax) with respect to the input. Shape should be the\n",
    "              same shape as x_value_batch.\n",
    "      call_model_args: The arguments that will be passed to the call model\n",
    "        function, for every call of the model.\n",
    "    \"\"\"\n",
    "    x_value_batched = np.expand_dims(x_value, axis=0)\n",
    "    call_model_output = call_model_function(\n",
    "        x_value_batched,\n",
    "        call_model_args=call_model_args,\n",
    "        expected_keys=self.expected_keys)\n",
    "\n",
    "    # Check gradient calculation is correct\n",
    "    self.format_and_check_call_model_output(call_model_output,\n",
    "                                            x_value_batched.shape,\n",
    "                                            self.expected_keys)\n",
    "\n",
    "    return call_model_output[INPUT_OUTPUT_GRADIENTS][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# Apply the function to each image path in the 'path' column of the dataframe\n",
    "df['image'] = df['path'].apply(lambda x: read_and_resize_image(x, image_size))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T11:28:13.089639800Z",
     "start_time": "2023-05-13T11:26:40.753092100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "image\n(64, 64, 3)    10015\nName: count, dtype: int64"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['image'].map(lambda x: x.shape).value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T11:28:13.108590700Z",
     "start_time": "2023-05-13T11:28:13.091589100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "# We use stratify which splits the dataset with the same class inbalance as the dataset.\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['cell_type_idx'], random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T11:28:13.166254600Z",
     "start_time": "2023-05-13T11:28:13.107600600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# Extract the image data and target labels for train and test sets\n",
    "X_train = np.stack(train_df['image'].values)\n",
    "y_train = train_df['cell_type_idx'].values\n",
    "X_test = np.stack(test_df['image'].values)\n",
    "y_test = test_df['cell_type_idx'].values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T11:28:13.259251500Z",
     "start_time": "2023-05-13T11:28:13.139250200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(image_size[1], image_size[0], 3)))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(df['cell_type_idx'].unique()), activation='softmax'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T11:28:13.510902300Z",
     "start_time": "2023-05-13T11:28:13.248251700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss= 'sparse_categorical_crossentropy', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T11:28:13.526888300Z",
     "start_time": "2023-05-13T11:28:13.512858800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 13s 749ms/step - loss: 36.8155 - accuracy: 0.5027 - val_loss: 1.2608 - val_accuracy: 0.6670\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 12s 771ms/step - loss: 1.1406 - accuracy: 0.6508 - val_loss: 1.0390 - val_accuracy: 0.6665\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 12s 788ms/step - loss: 0.9865 - accuracy: 0.6730 - val_loss: 0.9527 - val_accuracy: 0.6815\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 13s 804ms/step - loss: 0.9132 - accuracy: 0.6927 - val_loss: 0.9597 - val_accuracy: 0.6737\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 12s 778ms/step - loss: 0.8852 - accuracy: 0.7029 - val_loss: 0.9523 - val_accuracy: 0.6802\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 13s 792ms/step - loss: 0.8481 - accuracy: 0.7124 - val_loss: 0.9349 - val_accuracy: 0.6780\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 12s 779ms/step - loss: 0.7823 - accuracy: 0.7324 - val_loss: 0.8973 - val_accuracy: 0.6922\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 12s 748ms/step - loss: 0.7659 - accuracy: 0.7339 - val_loss: 0.9217 - val_accuracy: 0.6882\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 12s 768ms/step - loss: 0.7182 - accuracy: 0.7471 - val_loss: 0.9235 - val_accuracy: 0.6902\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 15s 942ms/step - loss: 0.7171 - accuracy: 0.7539 - val_loss: 0.9712 - val_accuracy: 0.6890\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x2311568baf0>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=256, epochs=10, validation_split=0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T11:30:19.600734300Z",
     "start_time": "2023-05-13T11:28:13.528858800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metric ouputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 2s 24ms/step\n",
      "F1 score: 0.63\n",
      "Precision: 0.61\n",
      "Recall: 0.68\n",
      "Accuracy: 0.68\n"
     ]
    }
   ],
   "source": [
    "output_metrics(model, X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T11:30:21.441120300Z",
     "start_time": "2023-05-13T11:30:19.578739100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# Create a new DataFrame with 'lesion_id' and 'cell_type_dx' columns from the test data\n",
    "result_df = pd.DataFrame({'lesion_id': test_df['lesion_id'], 'target': y_test})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T11:30:21.456089300Z",
     "start_time": "2023-05-13T11:30:21.444087800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualisation Application"
   ],
   "metadata": {
    "id": "nXV_484h8qBm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "heatmap_images = []\n",
    "raw_gradients = []"
   ],
   "metadata": {
    "id": "SazY4Jp63LlD",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:30:21.492122900Z",
     "start_time": "2023-05-13T11:30:21.459081700Z"
    }
   },
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "last_conv_layer = model.get_layer(index=-3)\n",
    "new_model = tf.keras.models.Model(inputs=model.input, \n",
    "                                   outputs=[last_conv_layer.output, model.output])"
   ],
   "metadata": {
    "id": "hggXkmPO3DYK",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:30:21.863501900Z",
     "start_time": "2023-05-13T11:30:21.474091900Z"
    }
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "smoothgrad = Gradients()"
   ],
   "metadata": {
    "id": "_nfiPssg9rfe",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:30:21.898501700Z",
     "start_time": "2023-05-13T11:30:21.863501900Z"
    }
   },
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "images = df['path']"
   ],
   "metadata": {
    "id": "XWJbnZ2dKKQP",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:30:21.961502800Z",
     "start_time": "2023-05-13T11:30:21.871498700Z"
    }
   },
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from PIL import Image"
   ],
   "metadata": {
    "id": "CkRYNrV5Oath",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:30:21.963497700Z",
     "start_time": "2023-05-13T11:30:21.905504Z"
    }
   },
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 2s 34ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T11:30:24.282176900Z",
     "start_time": "2023-05-13T11:30:21.919500300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "baseline = np.zeros(image_size)\n",
    "prediction_class = np.argmax(predictions[0])\n",
    "call_model_args = {class_idx_str: prediction_class}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T11:30:24.431252Z",
     "start_time": "2023-05-13T11:30:24.284176800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for img in images[:100]:\n",
    "  img_arr = read_and_resize_image(image_path=img, size=image_size)\n",
    "  im_tensor = PreprocessImages([img_arr])\n",
    "  im = img_arr.astype(np.float32)\n",
    "  vanilla_integrated_gradients_mask_3d = smoothgrad.GetSmoothedMask(\n",
    "    im, call_model_function, call_model_args)\n",
    "  raw_gradients.append(vanilla_integrated_gradients_mask_3d)\n",
    "  heatmap_images.append(visualiseImageToHeatmap(vanilla_integrated_gradients_mask_3d))"
   ],
   "metadata": {
    "id": "fe1WZKB92AuA",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f07c0b01-a900-44a8-e5da-059633c69b43",
    "ExecuteTime": {
     "end_time": "2023-05-13T11:34:13.383166700Z",
     "start_time": "2023-05-13T11:33:24.147667800Z"
    }
   },
   "execution_count": 57,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "folder_path = 'content/HAM10000_segmentations_lesion_tschandl/HAM10000_segmentations_lesion_tschandl'\n",
    "image_list = load_images_from_folder(folder_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T11:34:13.922256600Z",
     "start_time": "2023-05-13T11:34:13.386167500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "segment_images = load_images_from_folder(folder_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T11:34:14.576164Z",
     "start_time": "2023-05-13T11:34:13.924260Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "boolean_masks = []\n",
    "for img in image_list:\n",
    "    boolean_masks.append(convert_to_boolean_mask(img))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T12:31:46.207680600Z",
     "start_time": "2023-05-13T12:31:34.524780200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "overlap = []\n",
    "for i in range(len(raw_gradients)):\n",
    "    overlap.append([calculate_overlap(raw_gradients[i][:, :, 0], boolean_masks[i]), np.argmax(predictions, axis=1)[i]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T12:33:03.713782Z",
     "start_time": "2023-05-13T12:33:03.671023500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Calculate average overlap for each class\n",
    "class_overlaps = defaultdict(list)\n",
    "for item in overlap:\n",
    "    overlap_value = item[0]\n",
    "    class_label = item[1]\n",
    "    class_overlaps[class_label].append(overlap_value)\n",
    "\n",
    "class_labels = []\n",
    "average_overlaps = []\n",
    "\n",
    "for class_label, overlaps in class_overlaps.items():\n",
    "    class_labels.append(class_label)\n",
    "    average_overlap = sum(overlaps) / len(overlaps)\n",
    "    average_overlaps.append(average_overlap)\n",
    "\n",
    "# Create a bar chart or scatter plot\n",
    "plt.figure(figsize=(10, 6))  # Adjust the figure size as per your preference\n",
    "\n",
    "mapped_labels = [list(lesion_type_dict.values())[label] for label in class_labels]\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "# Bar chart\n",
    "plt.bar(range(len(average_overlaps)), average_overlaps)\n",
    "plt.xticks(range(len(average_overlaps)), mapped_labels)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Average Overlap')\n",
    "plt.title('Average Overlap of Gradients over Masks')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "gpuType": "T4",
   "collapsed_sections": [
    "CgNdhpLOvFjm",
    "Q7A6B6RebmmE",
    "FpxPXbXphKid"
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
