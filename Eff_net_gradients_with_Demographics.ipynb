{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toby-j/COM3025-CW/blob/main/Eff_net_gradients_with_Demographics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgNdhpLOvFjm"
      },
      "source": [
        "# Imports\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qDkEGwmWu8Bx",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.119677700Z",
          "start_time": "2023-05-13T11:26:33.840438300Z"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "from keras import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import PIL.Image\n",
        "from torchvision import models, transforms\n",
        "from matplotlib import pylab as P\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorflow\n",
        "from keras.applications import EfficientNetB0\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "from keras.regularizers import l2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7A6B6RebmmE"
      },
      "source": [
        "\n",
        "# Global functions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "collapsed": false,
        "id": "Ga5fFxodc5GZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss(history):\n",
        "  # Plotting the loss graph\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(history.history['loss'], label='Training Loss')\n",
        "  plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  # Plotting the accuracy graph\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "  plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "  plt.title('Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "cixTYO4qYoad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(model, X_test, y_test):\n",
        "  y_pred = model.predict(X_test)\n",
        "  y_pred_classes = np.argmax(y_pred, axis=-1)\n",
        "  \n",
        "  # Filter out continuous-multioutput targets from y_test\n",
        "  y_test_classes = np.argmax(y_test, axis=-1)\n",
        "  \n",
        "  confusion_matrix = metrics.confusion_matrix(y_test_classes, y_pred_classes)\n",
        "\n",
        "  labels = list(set(y_test_classes) | set(y_pred_classes))\n",
        "  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=labels)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(10, 8))\n",
        "  cm_display.plot(ax=ax)\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.154694900Z",
          "start_time": "2023-05-13T11:26:40.123676300Z"
        },
        "id": "xnjaiTnGc5GZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jxu5s9dbp9P",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.175693500Z",
          "start_time": "2023-05-13T11:26:40.139694100Z"
        }
      },
      "outputs": [],
      "source": [
        "def output_metrics(model, X_test, y_test):\n",
        "    # Use the trained model to make predictions on the test data\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=-1)\n",
        "    y_test = np.argmax(y_test, axis=-1)\n",
        "    # Calculate F1 score\n",
        "    f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "\n",
        "    # Calculate precision\n",
        "    precision = precision_score(y_test, y_pred_classes, average='weighted')\n",
        "\n",
        "    # Calculate recall\n",
        "    recall = recall_score(y_test, y_pred_classes, average='weighted')\n",
        "\n",
        "    # Print the metrics\n",
        "    print(f'F1 score: {f1:.2f}')\n",
        "    print(f'Precision: {precision:.2f}')\n",
        "    print(f'Recall: {recall:.2f}')\n",
        "    print(f'Accuracy: {accuracy:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def output_MLP_metrics(model, data, labels):\n",
        "  predictions = model.predict(data)\n",
        "\n",
        "  y_pred  = np.argmax(predictions, axis=-1)\n",
        "\n",
        "  print(f'Accuracy score: {accuracy_score(labels, y_pred)}')\n",
        "  print(f'F1 score: {f1_score(labels, y_pred, average=\"weighted\")}')\n",
        "  print(f'Precision score: {precision_score(labels, y_pred, average=\"weighted\")}')\n",
        "  print(f'Recall score: {recall_score(labels, y_pred, average=\"weighted\")}')"
      ],
      "metadata": {
        "id": "tNZL01UBo_-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_MLP_confusion_matrix(model, X_test, y_test):\n",
        "  y_pred = model.predict(X_test)\n",
        "  y_pred_classes = np.argmax(y_pred, axis=-1)\n",
        "  \n",
        "  confusion_matrix = metrics.confusion_matrix(y_test, y_pred_classes)\n",
        "\n",
        "  labels = list(set(y_test) | set(y_pred_classes))\n",
        "  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=labels)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(10, 8))\n",
        "  cm_display.plot(ax=ax)\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "evNP2iCVpQtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing"
      ],
      "metadata": {
        "collapsed": false,
        "id": "8DmNjb8xc5GZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def load_images_from_folder(folder_path):\n",
        "    image_paths = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        img_path = os.path.join(folder_path, filename)\n",
        "        if os.path.isfile(img_path):\n",
        "            image_paths.append(img_path)\n",
        "    return image_paths"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.176728100Z",
          "start_time": "2023-05-13T11:26:40.155692700Z"
        },
        "id": "cWYuYQz-c5GZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def read_and_resize_image(image_path, size):\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.resize(img, size)\n",
        "    return img"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.203697200Z",
          "start_time": "2023-05-13T11:26:40.168692900Z"
        },
        "id": "EsiNHATJc5GZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def normaliseGradients(image_3d, percentile=99):\n",
        "    image_2d = np.sum(np.abs(image_3d), axis=2)\n",
        "\n",
        "    # Get max pixel value in the image\n",
        "    vmax = np.percentile(image_2d, percentile)\n",
        "    # Get minimum pixel value in the image\n",
        "    vmin = np.min(image_2d)\n",
        "\n",
        "    # Normalise the values. We clip intensities so values lower than 0 are equal 0.\n",
        "    return np.clip((image_2d - vmin) / (vmax - vmin), 0, 1)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.253719400Z",
          "start_time": "2023-05-13T11:26:40.186692800Z"
        },
        "id": "qatwwSwVc5Ga"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def convert_to_boolean_mask(image):\n",
        "    # Convert the image to a NumPy array\n",
        "    image = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Create a boolean mask where white pixels are True and black pixels are False\n",
        "    binary_image = np.where(image == 255, False, True)  # Assuming white pixels are represented as 255\n",
        "\n",
        "    cropped_image = cv2.resize(binary_image.astype(np.uint8), image_size)\n",
        "\n",
        "    return cropped_image"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.253719400Z",
          "start_time": "2023-05-13T11:26:40.201698Z"
        },
        "id": "5kRkd7Zvc5Ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculations"
      ],
      "metadata": {
        "collapsed": false,
        "id": "R04qroeIc5Ga"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def calculate_overlap(g, m):\n",
        "    # Invert the mask, so the pixels outside are True.\n",
        "    # Replace where the mask is False, with a 0 in the same location in raw_gradients\n",
        "    segment = np.where(np.array(m), np.array(g), 0)\n",
        "\n",
        "    # We now have just the gradients in a 2D vector of the pixels outside the bounding box\n",
        "    sum_mask_segment = np.sum(segment)\n",
        "    # Find what percentage the outside pixels make up of the full gradient image by summing both 2D vectors\n",
        "    total_sum = np.sum(g)\n",
        "    # What percentage are the gradients outside the segment of the full gradient vector\n",
        "    overlap = (sum_mask_segment / total_sum) * 100\n",
        "\n",
        "    return overlap"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T12:32:24.830687900Z",
          "start_time": "2023-05-13T12:32:24.775654200Z"
        },
        "id": "Aghw834qc5Ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilities"
      ],
      "metadata": {
        "collapsed": false,
        "id": "3wlHWquEc5Ga"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGgdn5RUfZ-V",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:38:31.397293800Z",
          "start_time": "2023-05-13T11:38:31.383282900Z"
        }
      },
      "outputs": [],
      "source": [
        "def ShowGrayscaleImage(im, title='', ax=None):\n",
        "  if ax is None:\n",
        "    P.figure()\n",
        "  P.axis('off')\n",
        "\n",
        "  P.imshow(im, cmap=P.cm.gray, vmin=0, vmax=1)\n",
        "  P.title(title)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "def PreprocessImages(images):\n",
        "    # assumes input is 4-D, with range [0,255]\n",
        "    #\n",
        "    # torchvision have color channel as first dimension\n",
        "    # with normalization relative to mean/std of ImageNet:\n",
        "    #    https://pytorch.org/vision/stable/models.html\n",
        "    images = np.array(images)\n",
        "    images = images/255\n",
        "    images = np.transpose(images, (0,3,1,2))\n",
        "    images = torch.tensor(images, dtype=torch.float32)\n",
        "    images = transformer.forward(images)\n",
        "    return images.requires_grad_(True)"
      ],
      "metadata": {
        "id": "10o0c6Nxwvdn",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:38:31.417797500Z",
          "start_time": "2023-05-13T11:38:31.397293800Z"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualiseImageToHeatmap(image_3d, percentile=99):\n",
        "    r\"\"\"Returns a 3D tensor as RGB 3D heatmap\n",
        "    Pixels with higher weightage in sailiency heatmap will most saturated and will correspond to high RGB values in output heatmap_rgb\n",
        "  \"\"\"\n",
        "    image_2d = normaliseGradients(image_3d)\n",
        "    # Create heatmap using \"jet\" colormap, which returns an RGBA image\n",
        "    heatmap = plt.get_cmap('jet')(image_2d) * 255\n",
        "\n",
        "    # Normalise to 0,255 so it's visible when pasted\n",
        "    return Image.fromarray(heatmap.astype(np.uint8), mode='RGBA'), image_2d"
      ],
      "metadata": {
        "id": "1Eifbs32w2CP",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:38:31.447795700Z",
          "start_time": "2023-05-13T11:38:31.413796800Z"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LoadImage(file_path):\n",
        "    im = PIL.Image.open(file_path)\n",
        "    im = im.resize((299, 299))\n",
        "    im = np.asarray(im)\n",
        "    return im"
      ],
      "metadata": {
        "id": "bH5P4ljFw-GB",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:38:31.447795700Z",
          "start_time": "2023-05-13T11:38:31.427797900Z"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ShowImage(im, title='', ax=None):\n",
        "    if ax is None:\n",
        "        P.figure()\n",
        "    P.axis('off')\n",
        "    P.imshow(im)\n",
        "    P.title(title)"
      ],
      "metadata": {
        "id": "YXpx3NuHxHU7",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:38:31.466769200Z",
          "start_time": "2023-05-13T11:38:31.444798200Z"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global Variables"
      ],
      "metadata": {
        "collapsed": false,
        "id": "H9Z0wEw2c5Gb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# This variable is used to globally set the size of the images used for training and viaulisation.\n",
        "# All images must be the same size for the techniques to work.\n",
        "image_size = (128, 128)\n",
        "lesion_segment_path = \"/content/HAM10000_segmentations_lesion_tschandl/\""
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.340720400Z",
          "start_time": "2023-05-13T11:26:40.310698700Z"
        },
        "id": "Ge0Mh2aUc5Gb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwaor_SUveCb"
      },
      "source": [
        "# Kaggle dataset loading\n",
        "If you're using your local machine, download the dataset into root/contents\n",
        "Kaggle doesn't have the segmentation images. Download this folder from the Harvard dataverse and place in /contents:\n",
        "\n",
        "https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T\n",
        "https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000\n",
        "\n",
        "You need a kaggle.json api key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qp8DNrdxvi97",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.340720400Z",
          "start_time": "2023-05-13T11:26:40.327692200Z"
        }
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d kmader/skin-cancer-mnist-ham10000\n",
        "!unzip -q skin-cancer-mnist-ham10000.zip -d content\n",
        "#Removing the zip to save space\n",
        "!rm skin-cancer-mnist-ham10000.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CUH6AAsSJ6Eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading segment images from Drive"
      ],
      "metadata": {
        "id": "ndlmh-OXSJbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/drive/MyDrive/HAM10000_segmentations_lesion_tschandl.zip -d content"
      ],
      "metadata": {
        "id": "7_ay8FiASP_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek5Dfin9Svmd"
      },
      "source": [
        "## Dataset cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEzN-8pAu8Bz",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.426306900Z",
          "start_time": "2023-05-13T11:26:40.341701Z"
        }
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('content/HAM10000_metadata.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41stH92zu8B-",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.426306900Z",
          "start_time": "2023-05-13T11:26:40.374694500Z"
        }
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6CC22ABu8CB",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.426306900Z",
          "start_time": "2023-05-13T11:26:40.407278800Z"
        }
      },
      "outputs": [],
      "source": [
        "lesion_type_dict = {\n",
        "    'nv': 'Melanocytic nevi',\n",
        "    'mel': 'Melanoma',\n",
        "    'bkl': 'Bening keratosis-like lesions',\n",
        "    'bcc': 'Basal cell carcinoma',\n",
        "    'akiec': 'Actinic keratoses',\n",
        "    'vasc': 'Vascular lesions',\n",
        "    'df': 'Dermatofibroma'\n",
        "}\n",
        "ds_dir = 'content/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-i_Kyq9u8CC",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.608819700Z",
          "start_time": "2023-05-13T11:26:40.423278500Z"
        }
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "## Let's map the image_id with it's image path from part 1 and part 2 folders\n",
        "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x\n",
        "                     for x in glob(os.path.join(ds_dir, '*', '*.jpg'))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge6Zi_Plu8CK",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.609805900Z",
          "start_time": "2023-05-13T11:26:40.553799Z"
        }
      },
      "outputs": [],
      "source": [
        "df['path'] = df['image_id'].map(imageid_path_dict.get)\n",
        "df['cell_type'] = df['dx'].map(lesion_type_dict.get)\n",
        "df['cell_type_idx'] = pd.Categorical(df['cell_type']).codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0WLC4Dnu8CT",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.609805900Z",
          "start_time": "2023-05-13T11:26:40.554800200Z"
        }
      },
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Fill in the null values with the average age\n",
        "df['age'].fillna((df['age'].mean()), inplace=True)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.610827500Z",
          "start_time": "2023-05-13T11:26:40.567798900Z"
        },
        "id": "dnQwzdIyc5Gc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Check our dataset is cleaned for null values\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.610827500Z",
          "start_time": "2023-05-13T11:26:40.579805900Z"
        },
        "id": "CUokdrbGc5Gc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JElKkmMj_8j"
      },
      "source": [
        "# Smoothgrad\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiuvG8mOsNqk",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.610827500Z",
          "start_time": "2023-05-13T11:26:40.595801700Z"
        }
      },
      "outputs": [],
      "source": [
        "class_idx_str = 'class_idx_str'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0FkwjCJs8_n",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.670089300Z",
          "start_time": "2023-05-13T11:26:40.612800700Z"
        }
      },
      "outputs": [],
      "source": [
        "INPUT_OUTPUT_GRADIENTS = 'INPUT_OUTPUT_GRADIENTS'\n",
        "CONVOLUTION_LAYER_VALUES = 'CONVOLUTION_LAYER_VALUES'\n",
        "CONVOLUTION_OUTPUT_GRADIENTS = 'CONVOLUTION_OUTPUT_GRADIENTS'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sa6perPzzK4E",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.757088500Z",
          "start_time": "2023-05-13T11:26:40.629089900Z"
        }
      },
      "outputs": [],
      "source": [
        "expected_keys = [INPUT_OUTPUT_GRADIENTS]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMKDQNRNsekV",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.757088500Z",
          "start_time": "2023-05-13T11:26:40.645089100Z"
        }
      },
      "outputs": [],
      "source": [
        "conv_layer_outputs = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XQ7SBxWqcSd",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.758087800Z",
          "start_time": "2023-05-13T11:26:40.666089900Z"
        }
      },
      "outputs": [],
      "source": [
        "def call_model_function(images, call_model_args=None, expected_keys=None):\n",
        "    target_class_idx =  call_model_args[class_idx_str]\n",
        "    images = tf.convert_to_tensor(images)\n",
        "    with tf.GradientTape() as tape:\n",
        "        if expected_keys==[INPUT_OUTPUT_GRADIENTS]:\n",
        "            tape.watch(images)\n",
        "            _, output_layer = new_model(images)\n",
        "            output_layer = output_layer[:,target_class_idx]\n",
        "            gradients = np.array(tape.gradient(output_layer, images))\n",
        "            return {INPUT_OUTPUT_GRADIENTS: gradients}\n",
        "        else:\n",
        "            conv_layer, output_layer = new_model(images)\n",
        "            gradients = np.array(tape.gradient(output_layer, conv_layer))\n",
        "            return {CONVOLUTION_LAYER_VALUES: conv_layer,\n",
        "                    CONVOLUTION_OUTPUT_GRADIENTS: gradients}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mbaFPIGOiYU",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.758087800Z",
          "start_time": "2023-05-13T11:26:40.696091100Z"
        }
      },
      "outputs": [],
      "source": [
        "def format_and_check_call_model_output(self, output, input_shape, expected_keys):\n",
        "  \"\"\"Converts keys in the output into an np.ndarray, and confirms its shape.\n",
        "\n",
        "  Args:\n",
        "    output: The output dictionary of data to be formatted.\n",
        "    input_shape: The shape of the input that yielded the output\n",
        "    expected_keys: List of keys inside output to format/check for shape agreement.\n",
        "\n",
        "  Raises:\n",
        "      ValueError: If output shapes do not match expected shape.\"\"\"\n",
        "  # If key is in check_full_shape, the shape should be equal to the input shape (e.g. \n",
        "  # INPUT_OUTPUT_GRADIENTS, which gives gradients for each value of the input). Otherwise,\n",
        "  # only checks the outermost dimension of output to match input_shape (i.e. the batch size\n",
        "  # should be the same).\n",
        "  check_full_shape = [INPUT_OUTPUT_GRADIENTS]\n",
        "  for expected_key in expected_keys:\n",
        "    output[expected_key] = np.asarray(output[expected_key])\n",
        "    expected_shape = input_shape\n",
        "    actual_shape = output[expected_key].shape\n",
        "    if expected_key not in check_full_shape:\n",
        "      expected_shape = expected_shape[0]\n",
        "      actual_shape = actual_shape[0]\n",
        "    if expected_shape != actual_shape:\n",
        "      raise ValueError(SHAPE_ERROR_MESSAGE[expected_key].format(\n",
        "                      expected_shape, actual_shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzMDaanyQOa4",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.759087600Z",
          "start_time": "2023-05-13T11:26:40.715092300Z"
        }
      },
      "outputs": [],
      "source": [
        "# Output of the last convolution layer for the given input, including the batch\n",
        "# dimension.\n",
        "CONVOLUTION_LAYER_VALUES = 'CONVOLUTION_LAYER_VALUES'\n",
        "# Gradients of the output being explained (the logit/softmax value) with respect\n",
        "# to the last convolution layer, including the batch dimension.\n",
        "CONVOLUTION_OUTPUT_GRADIENTS = 'CONVOLUTION_OUTPUT_GRADIENTS'\n",
        "# Gradients of the output being explained (the logit/softmax value) with respect\n",
        "# to the input. Shape should be the same shape as x_value_batch.\n",
        "INPUT_OUTPUT_GRADIENTS = 'INPUT_OUTPUT_GRADIENTS'\n",
        "# Value of the output being explained (the logit/softmax value).\n",
        "OUTPUT_LAYER_VALUES = 'OUTPUT_LAYER_VALUES'\n",
        "\n",
        "SHAPE_ERROR_MESSAGE = {\n",
        "    CONVOLUTION_LAYER_VALUES: (\n",
        "        'Expected outermost dimension of CONVOLUTION_LAYER_VALUES to be the '\n",
        "        'same as x_value_batch - expected {}, actual {}'\n",
        "    ),\n",
        "    CONVOLUTION_OUTPUT_GRADIENTS: (\n",
        "        'Expected outermost dimension of CONVOLUTION_OUTPUT_GRADIENTS to be the '\n",
        "        'same as x_value_batch - expected {}, actual {}'\n",
        "    ),\n",
        "    INPUT_OUTPUT_GRADIENTS: (\n",
        "        'Expected key INPUT_OUTPUT_GRADIENTS to be the same shape as input '\n",
        "        'x_value_batch - expected {}, actual {}'\n",
        "    ),\n",
        "    OUTPUT_LAYER_VALUES: (\n",
        "        'Expected outermost dimension of OUTPUT_LAYER_VALUES to be the same as'\n",
        "        ' x_value_batch - expected {}, actual {}'\n",
        "    ),\n",
        "}\n",
        "\n",
        "\n",
        "class CoreGradients(object):\n",
        "\n",
        "  def GetMask(self, x_value, call_model_function, call_model_args=None):\n",
        "    \"\"\"Returns an unsmoothed mask.\n",
        "\n",
        "    Args:\n",
        "      x_value: Input ndarray.\n",
        "      call_model_function: A function that interfaces with a model to return\n",
        "        specific output in a dictionary when given an input and other arguments.\n",
        "        Expected function signature:\n",
        "        - call_model_function(x_value_batch,\n",
        "                              call_model_args=None,\n",
        "                              expected_keys=None):\n",
        "          x_value_batch - Input for the model, given as a batch (i.e. dimension\n",
        "            0 is the batch dimension, dimensions 1 through n represent a single\n",
        "            input).\n",
        "          call_model_args - Other arguments used to call and run the model.\n",
        "          expected_keys - List of keys that are expected in the output. Possible\n",
        "            keys in this list are CONVOLUTION_LAYER_VALUES, \n",
        "            CONVOLUTION_OUTPUT_GRADIENTS, INPUT_OUTPUT_GRADIENTS, and\n",
        "            OUTPUT_LAYER_VALUES, and are explained in detail where declared.\n",
        "      call_model_args: The arguments that will be passed to the call model\n",
        "        function, for every call of the model.\n",
        "\n",
        "    \"\"\"\n",
        "    raise NotImplementedError('A derived class should implemented GetMask()')\n",
        "\n",
        "  def GetSmoothedMask(self,\n",
        "                      x_value,\n",
        "                      call_model_function,\n",
        "                      call_model_args=None,\n",
        "                      stdev_spread=.15,\n",
        "                      nsamples=25,\n",
        "                      magnitude=True,\n",
        "                      **kwargs):\n",
        "    \"\"\"Returns a mask that is smoothed with the SmoothGrad method.\n",
        "\n",
        "    Args:\n",
        "      x_value: Input ndarray.\n",
        "      call_model_function: A function that interfaces with a model to return\n",
        "        specific output in a dictionary when given an input and other arguments.\n",
        "        Expected function signature:\n",
        "        - call_model_function(x_value_batch,\n",
        "                              call_model_args=None,\n",
        "                              expected_keys=None):\n",
        "          x_value_batch - Input for the model, given as a batch (i.e. dimension\n",
        "            0 is the batch dimension, dimensions 1 through n represent a single\n",
        "            input).\n",
        "          call_model_args - Other arguments used to call and run the model.\n",
        "          expected_keys - List of keys that are expected in the output. Possible\n",
        "            keys in this list are CONVOLUTION_LAYER_VALUES,\n",
        "            CONVOLUTION_OUTPUT_GRADIENTS, INPUT_OUTPUT_GRADIENTS, and\n",
        "            OUTPUT_LAYER_VALUES, and are explained in detail where declared.\n",
        "      call_model_args: The arguments that will be passed to the call model\n",
        "        function, for every call of the model.\n",
        "      stdev_spread: Amount of noise to add to the input, as fraction of the\n",
        "                    total spread (x_max - x_min). Defaults to 15%.\n",
        "      nsamples: Number of samples to average across to get the smooth gradient.\n",
        "      magnitude: If true, computes the sum of squares of gradients instead of\n",
        "                 just the sum. Defaults to true.\n",
        "    \"\"\"\n",
        "    stdev = stdev_spread * (np.max(x_value) - np.min(x_value))\n",
        "    # Starting baseline image\n",
        "    total_gradients = np.zeros_like(x_value, dtype=np.float32)\n",
        "    for _ in range(nsamples):\n",
        "      noise = np.random.normal(0, stdev, x_value.shape)\n",
        "      # Calculate and add our smoothgrad noise\n",
        "      x_plus_noise = x_value + noise\n",
        "      # Get vanilla gradients. The input is the interpolated image + the Smoothgrad noise we generated\n",
        "      grad = self.GetMask(x_plus_noise, call_model_function, call_model_args,\n",
        "                          **kwargs)\n",
        "      if magnitude:\n",
        "        total_gradients += (grad * grad)\n",
        "      else:\n",
        "        total_gradients += grad\n",
        "\n",
        "    return total_gradients / nsamples\n",
        "\n",
        "  def format_and_check_call_model_output(self, output, input_shape, expected_keys):\n",
        "    \"\"\"Converts keys in the output into an np.ndarray, and confirms its shape.\n",
        "\n",
        "    Args:\n",
        "      output: The output dictionary of data to be formatted.\n",
        "      input_shape: The shape of the input that yielded the output\n",
        "      expected_keys: List of keys inside output to format/check for shape agreement.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If output shapes do not match expected shape.\"\"\"\n",
        "    # If key is in check_full_shape, the shape should be equal to the input shape (e.g. \n",
        "    # INPUT_OUTPUT_GRADIENTS, which gives gradients for each value of the input). Otherwise,\n",
        "    # only checks the outermost dimension of output to match input_shape (i.e. the batch size\n",
        "    # should be the same).\n",
        "    check_full_shape = [INPUT_OUTPUT_GRADIENTS]\n",
        "    for expected_key in expected_keys:\n",
        "      output[expected_key] = np.asarray(output[expected_key])\n",
        "      expected_shape = input_shape\n",
        "      actual_shape = output[expected_key].shape\n",
        "      if expected_key not in check_full_shape:\n",
        "        expected_shape = expected_shape[0]\n",
        "        actual_shape = actual_shape[0]\n",
        "      if expected_shape != actual_shape:\n",
        "        raise ValueError(SHAPE_ERROR_MESSAGE[expected_key].format(\n",
        "                       expected_shape, actual_shape))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMYQuC0onOFp",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.759087600Z",
          "start_time": "2023-05-13T11:26:40.741091100Z"
        }
      },
      "outputs": [],
      "source": [
        "# Inherits our smoothgrad technique in CoreGradients\n",
        "class Gradients(CoreGradients):\n",
        "\n",
        "  expected_keys = [INPUT_OUTPUT_GRADIENTS]\n",
        "\n",
        "  def GetMask(self, x_value, call_model_function, call_model_args=None):\n",
        "    \"\"\"Returns a vanilla gradients mask.\n",
        "\n",
        "    Args:\n",
        "      x_value: Input ndarray.\n",
        "      call_model_function: A function that interfaces with a model to return\n",
        "        specific data in a dictionary when given an input and other arguments.\n",
        "        Expected function signature:\n",
        "        - call_model_function(x_value_batch,\n",
        "                              call_model_args=None,\n",
        "                              expected_keys=None):\n",
        "          x_value_batch - Input for the model, given as a batch (i.e. dimension\n",
        "            0 is the batch dimension, dimensions 1 through n represent a single\n",
        "            input).\n",
        "          call_model_args - Other arguments used to call and run the model.\n",
        "          expected_keys - List of keys that are expected in the output. For this\n",
        "            method (Gradients), the expected keys are\n",
        "            INPUT_OUTPUT_GRADIENTS - Gradients of the output layer\n",
        "              (logit/softmax) with respect to the input. Shape should be the\n",
        "              same shape as x_value_batch.\n",
        "      call_model_args: The arguments that will be passed to the call model\n",
        "        function, for every call of the model.\n",
        "    \"\"\"\n",
        "    x_value_batched = np.expand_dims(x_value, axis=0)\n",
        "    call_model_output = call_model_function(\n",
        "        x_value_batched,\n",
        "        call_model_args=call_model_args,\n",
        "        expected_keys=self.expected_keys)\n",
        "\n",
        "    # Check gradient calculation is correct\n",
        "    self.format_and_check_call_model_output(call_model_output,\n",
        "                                            x_value_batched.shape,\n",
        "                                            self.expected_keys)\n",
        "\n",
        "    return call_model_output[INPUT_OUTPUT_GRADIENTS][0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "collapsed": false,
        "id": "1HXYqVgzc5Ge"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "outputs": [],
      "source": [
        "# Apply the function to each image path in the 'path' column of the dataframe\n",
        "df['image'] = df['path'].apply(lambda x: read_and_resize_image(x, image_size))"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:28:13.089639800Z",
          "start_time": "2023-05-13T11:26:40.753092100Z"
        },
        "id": "pSuK_BQRc5Ge"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 128, 3)    10015\n",
              "Name: image, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ],
      "source": [
        "df['image'].map(lambda x: x.shape).value_counts()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:28:13.108590700Z",
          "start_time": "2023-05-13T11:28:13.091589100Z"
        },
        "id": "mWewaFiJc5Ge",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2da8d8be-1b17-416f-ce34-45d4b201ed6f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "outputs": [],
      "source": [
        "# Split the data into train and test sets\n",
        "# We use stratify which splits the dataset with the same class inbalance as the dataset.\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['cell_type_idx'], random_state=42)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:28:13.166254600Z",
          "start_time": "2023-05-13T11:28:13.107600600Z"
        },
        "id": "Nmw8J2v3c5Ge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the image data and target labels for train and test sets\n",
        "X_train = np.stack(train_df['image'].values)\n",
        "y_train = train_df['cell_type_idx'].values\n",
        "X_test = np.stack(test_df['image'].values)\n",
        "y_test = test_df['cell_type_idx'].values"
      ],
      "metadata": {
        "id": "7BlXbH7ymvDV"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "outputs": [],
      "source": [
        "# Compute class weights\n",
        "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "# Create a data generator for data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        zoom_range=0.15,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.15,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode=\"nearest\")\n",
        "\n",
        "# Convert the labels to one-hot encoding for use with categorical_crossentropy\n",
        "num_classes = df['cell_type_idx'].nunique()\n",
        "y_train_norm = to_categorical(y_train, num_classes=num_classes)\n",
        "y_test_norm = to_categorical(y_test, num_classes=num_classes)\n",
        "\n",
        "# Normalizing the input data to match the format the model was trained on\n",
        "X_train_norm = X_train / 255.0\n",
        "X_test_norm = X_test / 255.0\n",
        "\n",
        "# Load the pre-trained EfficientNetB0 model\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(image_size[0], image_size[1], 3))\n",
        "\n",
        "# Add a custom head for classification\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(rate=0.2)(x)\n",
        "x = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(x)  # Added L2 regularization\n",
        "predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "# This is the model we will train\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze all layers, keeping Batch Normalization layers in inference mode\n",
        "for layer in base_model.layers:\n",
        "    if not isinstance(layer, tensorflow.keras.layers.BatchNormalization):\n",
        "        layer.trainable = False\n",
        "\n",
        "# Unfreeze the last few layers of the base model\n",
        "for layer in base_model.layers[-5:]:\n",
        "    layer.trainable = True  \n",
        "\n",
        "# Learning Rate Schedule\n",
        "lr_schedule = tensorflow.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.2, patience=5, min_lr=0.0010)\n",
        "\n",
        "# Early Stopping\n",
        "early_stopping = tensorflow.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=10, restore_best_weights=True)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:28:13.259251500Z",
          "start_time": "2023-05-13T11:28:13.139250200Z"
        },
        "id": "neRTUzAMc5Ge"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:28:13.526888300Z",
          "start_time": "2023-05-13T11:28:13.512858800Z"
        },
        "id": "gnthod6Vc5Gf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "125/125 [==============================] - 49s 217ms/step - loss: 5.3552 - accuracy: 0.4506 - val_loss: 4.6357 - val_accuracy: 0.0115 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "125/125 [==============================] - 28s 222ms/step - loss: 2.3271 - accuracy: 0.5318 - val_loss: 3.0814 - val_accuracy: 0.4658 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "125/125 [==============================] - 28s 223ms/step - loss: 1.6057 - accuracy: 0.5532 - val_loss: 2.7531 - val_accuracy: 0.0584 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "125/125 [==============================] - 28s 222ms/step - loss: 1.4298 - accuracy: 0.5593 - val_loss: 2.9264 - val_accuracy: 0.1053 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "125/125 [==============================] - 28s 225ms/step - loss: 1.2947 - accuracy: 0.5878 - val_loss: 2.0399 - val_accuracy: 0.3280 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "125/125 [==============================] - 28s 223ms/step - loss: 1.1999 - accuracy: 0.6125 - val_loss: 2.0866 - val_accuracy: 0.3540 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "125/125 [==============================] - 28s 223ms/step - loss: 1.1837 - accuracy: 0.6043 - val_loss: 1.4021 - val_accuracy: 0.5647 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "125/125 [==============================] - 28s 222ms/step - loss: 1.1179 - accuracy: 0.6217 - val_loss: 1.2401 - val_accuracy: 0.6261 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "125/125 [==============================] - 28s 222ms/step - loss: 1.1344 - accuracy: 0.6280 - val_loss: 1.3906 - val_accuracy: 0.5796 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "125/125 [==============================] - 28s 223ms/step - loss: 1.0665 - accuracy: 0.6341 - val_loss: 1.1846 - val_accuracy: 0.6276 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "125/125 [==============================] - 28s 222ms/step - loss: 1.0592 - accuracy: 0.6381 - val_loss: 1.5897 - val_accuracy: 0.5372 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "125/125 [==============================] - 28s 222ms/step - loss: 1.0573 - accuracy: 0.6454 - val_loss: 1.1672 - val_accuracy: 0.6211 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "125/125 [==============================] - 28s 223ms/step - loss: 0.9999 - accuracy: 0.6472 - val_loss: 1.1762 - val_accuracy: 0.6111 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "125/125 [==============================] - 28s 223ms/step - loss: 0.9634 - accuracy: 0.6623 - val_loss: 1.1545 - val_accuracy: 0.6141 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "125/125 [==============================] - 28s 223ms/step - loss: 0.9293 - accuracy: 0.6701 - val_loss: 1.6529 - val_accuracy: 0.4948 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "125/125 [==============================] - 28s 223ms/step - loss: 0.9405 - accuracy: 0.6614 - val_loss: 1.1888 - val_accuracy: 0.6301 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "120/125 [===========================>..] - ETA: 1s - loss: 0.9022 - accuracy: 0.6779"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-152-64415a7f9337>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = model.fit(train_datagen.flow(X_train_norm, y_train_norm, batch_size=64), \n\u001b[0m\u001b[1;32m      3\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_norm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#Train the model\n",
        "history = model.fit(train_datagen.flow(X_train_norm, y_train_norm, batch_size=64), \n",
        "          validation_data=(X_test_norm, y_test_norm), \n",
        "          class_weight=class_weights, \n",
        "          steps_per_epoch=len(X_train_norm) // 64,\n",
        "          epochs=50,\n",
        "          callbacks=[lr_schedule, early_stopping])"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:19.600734300Z",
          "start_time": "2023-05-13T11:28:13.528858800Z"
        },
        "id": "g4zcnIzoc5Gf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        },
        "outputId": "03cf3b41-afe7-4c2a-9602-7b0f6b88be2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metric ouputs"
      ],
      "metadata": {
        "collapsed": false,
        "id": "krdgxQIec5Gf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "output_metrics(model, X_test_norm, y_test_norm)\n",
        "plot_confusion_matrix(model, X_test_norm, y_test_norm)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:21.441120300Z",
          "start_time": "2023-05-13T11:30:19.578739100Z"
        },
        "id": "_tO-X_9xc5Gf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Create a new DataFrame with 'lesion_id' and 'cell_type_dx' columns from the test data\n",
        "result_df = pd.DataFrame({'lesion_id': test_df['lesion_id'], 'target': y_test})"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:21.456089300Z",
          "start_time": "2023-05-13T11:30:21.444087800Z"
        },
        "id": "iSI4DKTIc5Gf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualisation Application"
      ],
      "metadata": {
        "id": "nXV_484h8qBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "heatmap_images = []\n",
        "raw_gradients = []"
      ],
      "metadata": {
        "id": "SazY4Jp63LlD",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:21.492122900Z",
          "start_time": "2023-05-13T11:30:21.459081700Z"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the index of the last convolutional layer\n",
        "last_conv_index = None\n",
        "for i, layer in enumerate(model.layers[::-1]):\n",
        "    if 'conv' in layer.name:\n",
        "        last_conv_index = len(model.layers) - 1 - i\n",
        "        break\n",
        "\n",
        "if last_conv_index is not None:\n",
        "    # Select the last convolutional layer\n",
        "    last_conv_layer = model.get_layer(index=last_conv_index)\n",
        "\n",
        "    # Create a new model with the last convolutional layer as output\n",
        "    new_model = tf.keras.models.Model(inputs=model.input, outputs=[last_conv_layer.output, model.output])\n",
        "\n",
        "    # Print information about the selected layer\n",
        "    print(\"Selected layer name:\", last_conv_layer.name)\n",
        "    print(\"Selected layer output shape:\", last_conv_layer.output_shape)\n",
        "else:\n",
        "    print(\"No convolutional layer found in the model.\")"
      ],
      "metadata": {
        "id": "hggXkmPO3DYK",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:21.863501900Z",
          "start_time": "2023-05-13T11:30:21.474091900Z"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smoothgrad = Gradients()"
      ],
      "metadata": {
        "id": "_nfiPssg9rfe",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:21.898501700Z",
          "start_time": "2023-05-13T11:30:21.863501900Z"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = df['path']"
      ],
      "metadata": {
        "id": "XWJbnZ2dKKQP",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:21.961502800Z",
          "start_time": "2023-05-13T11:30:21.871498700Z"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "predictions = model.predict(X_test)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:24.282176900Z",
          "start_time": "2023-05-13T11:30:21.919500300Z"
        },
        "id": "1IM4aEync5Gg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "baseline = np.zeros(image_size)\n",
        "prediction_class = np.argmax(predictions[0])\n",
        "call_model_args = {class_idx_str: prediction_class}"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:24.431252Z",
          "start_time": "2023-05-13T11:30:24.284176800Z"
        },
        "id": "XxuyX0iOc5Gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty DataFrame to store the collected samples\n",
        "collected_samples = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "# Iterate over each unique class value\n",
        "for class_value in range(5):\n",
        "    # Filter the DataFrame to select samples with the current class value\n",
        "    class_samples = df[df['cell_type_idx'] == class_value].sample(n=100, random_state=42)\n",
        "    \n",
        "    # Append the selected samples to the collected_samples DataFrame\n",
        "    collected_samples = pd.concat([collected_samples, class_samples], ignore_index=True)"
      ],
      "metadata": {
        "id": "DdhB8yBeAtOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "images = collected_samples['path']\n",
        "\n",
        "progress_bar = tqdm(total=len(images))\n",
        "\n",
        "for img in images:\n",
        "  img_arr = read_and_resize_image(image_path=img, size=image_size)\n",
        "  im_tensor = PreprocessImages([img_arr])\n",
        "  im = img_arr.astype(np.float32)\n",
        "  vanilla_integrated_gradients_mask_3d = smoothgrad.GetSmoothedMask(\n",
        "    im, call_model_function, call_model_args)\n",
        "  raw_gradients.append(vanilla_integrated_gradients_mask_3d)\n",
        "  \n",
        "  # Update the progress bar\n",
        "  progress_bar.update(1)\n",
        "\n",
        "# Close the progress bar\n",
        "progress_bar.close()"
      ],
      "metadata": {
        "id": "fe1WZKB92AuA",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:34:13.383166700Z",
          "start_time": "2023-05-13T11:33:24.147667800Z"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segment_image_list = load_images_from_folder(\"/content/content/HAM10000_segmentations_lesion_tschandl\")"
      ],
      "metadata": {
        "id": "hm-D3L80RVoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "boolean_masks = []\n",
        "for img in segment_image_list:\n",
        "    boolean_masks.append(convert_to_boolean_mask(img))"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T12:31:46.207680600Z",
          "start_time": "2023-05-13T12:31:34.524780200Z"
        },
        "id": "yj5_y0_Jc5Gg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "overlap = []\n",
        "for i in range(len(images)):\n",
        "    per = [calculate_overlap(raw_gradients[i][:, :, 0], boolean_masks[i]), np.argmax(predictions, axis=1)[i]]\n",
        "    if str(per[0]) != \"nan\":\n",
        "      overlap.append(per)\n"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T12:33:03.713782Z",
          "start_time": "2023-05-13T12:33:03.671023500Z"
        },
        "id": "DbjKVuGVc5Gg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Calculate average overlap for each class\n",
        "class_overlaps = defaultdict(list)\n",
        "for item in overlap:\n",
        "    overlap_value = item[0]\n",
        "    class_label = item[1]\n",
        "    class_overlaps[class_label].append(overlap_value)\n",
        "\n",
        "class_labels = []\n",
        "average_overlaps = []\n",
        "\n",
        "for class_label, overlaps in class_overlaps.items():\n",
        "    class_labels.append(class_label)\n",
        "    average_overlap = sum(overlaps) / len(overlaps)\n",
        "    average_overlaps.append(average_overlap)\n",
        "\n",
        "# Create a bar chart or scatter plot\n",
        "plt.figure(figsize=(10, 6))  # Adjust the figure size as per your preference\n",
        "\n",
        "mapped_labels = [list(lesion_type_dict.values())[label] for label in class_labels]\n",
        "\n",
        "plt.figure(figsize=(20, 6))\n",
        "# Bar chart\n",
        "plt.bar(range(len(average_overlaps)), average_overlaps)\n",
        "plt.xticks(range(len(average_overlaps)), mapped_labels)\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Average Overlap')\n",
        "plt.title('Average Overlap of Gradients over Masks')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T12:34:59.508194500Z",
          "start_time": "2023-05-13T12:34:48.572324700Z"
        },
        "id": "biHVKI-Uc5Gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for label, overlap in zip(mapped_labels, average_overlaps):\n",
        "    print(f\"Label: {label}, Average Overlap: {overlap}\")"
      ],
      "metadata": {
        "id": "09ZgOYyG6VVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "plot_loss(history)"
      ],
      "metadata": {
        "id": "tOVxcIGqc5Gh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP"
      ],
      "metadata": {
        "id": "M_KhKcFZ_G73"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "EffNet = model\n",
        "from keras.layers import Input, concatenate\n",
        "\n",
        "def perceptron():\n",
        "  mlp = Sequential()\n",
        "  mlp.add(Input(shape=(3,)))\n",
        "  mlp.add(Flatten())\n",
        "  mlp.add(Dense(128, activation='relu'))\n",
        "  mlp.add(Dropout(0.5))\n",
        "  mlp.add(Dense(64, activation='relu'))\n",
        "  mlp.add(Dense(len(df['cell_type_idx'].unique()), activation='softmax'))\n",
        "  return mlp"
      ],
      "metadata": {
        "id": "OWw3_1ZypsCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron = perceptron()\n",
        "perceptron.compile(optimizer = 'adam', loss= 'sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "duVgHNta_KxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sex_dict = {\n",
        "    'male': 0.0,\n",
        "    'female': 1.0,\n",
        "    'unknown': 1.5\n",
        "}\n",
        "\n",
        "loc_dict = {\n",
        "    'back': 1.0,\n",
        "    'lower extremity': 2.0,\n",
        "    'trunk': 3.0,\n",
        "    'upper extremity': 4.0,\n",
        "    'abdomen': 5.0,\n",
        "    'face': 6.0,\n",
        "    'chest': 7.0,\n",
        "    'foot': 8.0,\n",
        "    'unknown': 9.0,\n",
        "    'neck': 10.0,\n",
        "    'scalp': 11.0,\n",
        "    'hand': 12.0,\n",
        "    'ear': 13.0,\n",
        "    'genital': 14.0,\n",
        "    'acral': 15.0\n",
        "}\n",
        "\n",
        "train_df = train_df.replace({\"sex\": sex_dict})\n",
        "train_df = train_df.replace({\"localization\": loc_dict})\n",
        "test_df = test_df.replace({\"sex\": sex_dict})\n",
        "test_df = test_df.replace({\"localization\": loc_dict})\n",
        "\n",
        "#Extract and recombine the demographic data for training set and test set\n",
        "X_train_demo = np.stack((np.asarray(train_df['age'].values), np.asarray(train_df['sex'].values), np.asarray(train_df['localization'].values)))\n",
        "y_train = train_df['cell_type_idx'].values\n",
        "X_test_demo = np.stack((np.asarray(test_df['age'].values), np.asarray(test_df['sex'].values), np.asarray(test_df['localization'].values)))\n",
        "y_test = test_df['cell_type_idx'].values\n",
        "\n",
        "X_train_demo=X_train_demo.T.astype(int)\n",
        "X_test_demo=X_test_demo.T.astype(int)"
      ],
      "metadata": {
        "id": "s7mjulC9ASEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron_history = perceptron.fit(X_train_demo, y_train, batch_size=256, epochs=50, validation_split=0.3)"
      ],
      "metadata": {
        "id": "ffiWyIHVAVDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the trained model to make predictions on the test data\n",
        "output_MLP_metrics(perceptron, X_test_demo, y_test)\n",
        "plot_MLP_confusion_matrix(perceptron, X_test_demo, y_test)\n",
        "plot_loss(perceptron_history)"
      ],
      "metadata": {
        "id": "rrQIs7KeAY7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concatenated_inputs = concatenate(inputs=[perceptron.output, EffNet.output])\n",
        "x = Dense(128, activation=\"relu\")(concatenated_inputs)\n",
        "x = Dense(64, activation=\"relu\")(x)\n",
        "x = Dense(len(df['cell_type_idx'].unique()), activation='softmax')(x)\n",
        "model = Model(inputs=[perceptron.input, EffNet.input], outputs=x)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Q5m8FslyAZVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_history = model.fit([X_train_demo, X_train], y_train, batch_size=256, epochs=50, validation_split=0.3)"
      ],
      "metadata": {
        "id": "0IOiBAbEAj4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_MLP_metrics(model, [X_test_demo, X_test], y_test)\n",
        "plot_MLP_confusion_matrix(model, [X_test_demo, X_test], y_test)\n",
        "plot_loss(model_history)"
      ],
      "metadata": {
        "id": "72yUPebpApIf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}