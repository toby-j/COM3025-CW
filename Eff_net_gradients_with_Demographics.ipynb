{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgNdhpLOvFjm"
      },
      "source": [
        "# Imports\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "qDkEGwmWu8Bx",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.119677700Z",
          "start_time": "2023-05-13T11:26:33.840438300Z"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "from keras import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import PIL.Image\n",
        "from torchvision import models, transforms\n",
        "from matplotlib import pylab as P\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorflow\n",
        "from keras.applications import EfficientNetB0\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "from keras.regularizers import l2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7A6B6RebmmE"
      },
      "source": [
        "\n",
        "# Global functions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "collapsed": false,
        "id": "Ga5fFxodc5GZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(model, X_test, y_test):\n",
        "  y_pred = model.predict(X_test)\n",
        "  y_pred_classes = np.argmax(y_pred, axis=-1)\n",
        "  \n",
        "  # Filter out continuous-multioutput targets from y_test\n",
        "  y_test_classes = np.argmax(y_test, axis=-1)\n",
        "  \n",
        "  confusion_matrix = metrics.confusion_matrix(y_test_classes, y_pred_classes)\n",
        "\n",
        "  labels = list(set(y_test_classes) | set(y_pred_classes))\n",
        "  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=labels)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(10, 8))\n",
        "  cm_display.plot(ax=ax)\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.154694900Z",
          "start_time": "2023-05-13T11:26:40.123676300Z"
        },
        "id": "xnjaiTnGc5GZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7jxu5s9dbp9P",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.175693500Z",
          "start_time": "2023-05-13T11:26:40.139694100Z"
        }
      },
      "outputs": [],
      "source": [
        "def output_metrics(model, X_test, y_test):\n",
        "    # Use the trained model to make predictions on the test data\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=-1)\n",
        "    y_test = np.argmax(y_test, axis=-1)\n",
        "    # Calculate F1 score\n",
        "    f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "\n",
        "    # Calculate precision\n",
        "    precision = precision_score(y_test, y_pred_classes, average='weighted')\n",
        "\n",
        "    # Calculate recall\n",
        "    recall = recall_score(y_test, y_pred_classes, average='weighted')\n",
        "\n",
        "    # Print the metrics\n",
        "    print(f'F1 score: {f1:.2f}')\n",
        "    print(f'Precision: {precision:.2f}')\n",
        "    print(f'Recall: {recall:.2f}')\n",
        "    print(f'Accuracy: {accuracy:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing"
      ],
      "metadata": {
        "collapsed": false,
        "id": "8DmNjb8xc5GZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "source": [
        "def load_images_from_folder(folder_path):\n",
        "    image_paths = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        img_path = os.path.join(folder_path, filename)\n",
        "        if os.path.isfile(img_path):\n",
        "            image_paths.append(img_path)\n",
        "    return image_paths"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.176728100Z",
          "start_time": "2023-05-13T11:26:40.155692700Z"
        },
        "id": "cWYuYQz-c5GZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "source": [
        "def read_and_resize_image(image_path, size):\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.resize(img, size)\n",
        "    return img"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.203697200Z",
          "start_time": "2023-05-13T11:26:40.168692900Z"
        },
        "id": "EsiNHATJc5GZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "source": [
        "def normaliseGradients(image_3d, percentile=99):\n",
        "    image_2d = np.sum(np.abs(image_3d), axis=2)\n",
        "\n",
        "    # Get max pixel value in the image\n",
        "    vmax = np.percentile(image_2d, percentile)\n",
        "    # Get minimum pixel value in the image\n",
        "    vmin = np.min(image_2d)\n",
        "\n",
        "    # Normalise the values. We clip intensities so values lower than 0 are equal 0.\n",
        "    return np.clip((image_2d - vmin) / (vmax - vmin), 0, 1)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.253719400Z",
          "start_time": "2023-05-13T11:26:40.186692800Z"
        },
        "id": "qatwwSwVc5Ga"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "source": [
        "def convert_to_boolean_mask(image):\n",
        "    # Convert the image to a NumPy array\n",
        "    image = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Create a boolean mask where white pixels are True and black pixels are False\n",
        "    binary_image = np.where(image == 255, False, True)  # Assuming white pixels are represented as 255\n",
        "\n",
        "    cropped_image = cv2.resize(binary_image.astype(np.uint8), image_size)\n",
        "\n",
        "    return cropped_image"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.253719400Z",
          "start_time": "2023-05-13T11:26:40.201698Z"
        },
        "id": "5kRkd7Zvc5Ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculations"
      ],
      "metadata": {
        "collapsed": false,
        "id": "R04qroeIc5Ga"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "source": [
        "def calculate_overlap(g, m):\n",
        "    # Invert the mask, so the pixels outside are True.\n",
        "    # Replace where the mask is False, with a 0 in the same location in raw_gradients\n",
        "    segment = np.where(np.array(m), np.array(g), 0)\n",
        "\n",
        "    # We now have just the gradients in a 2D vector of the pixels outside the bounding box\n",
        "    sum_mask_segment = np.sum(segment)\n",
        "    # Find what percentage the outside pixels make up of the full gradient image by summing both 2D vectors\n",
        "    total_sum = np.sum(g)\n",
        "    # What percentage are the gradients outside the segment of the full gradient vector\n",
        "    overlap = (sum_mask_segment / total_sum) * 100\n",
        "\n",
        "    return overlap"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T12:32:24.830687900Z",
          "start_time": "2023-05-13T12:32:24.775654200Z"
        },
        "id": "Aghw834qc5Ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilities"
      ],
      "metadata": {
        "collapsed": false,
        "id": "3wlHWquEc5Ga"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fGgdn5RUfZ-V",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:38:31.397293800Z",
          "start_time": "2023-05-13T11:38:31.383282900Z"
        }
      },
      "outputs": [],
      "source": [
        "def ShowGrayscaleImage(im, title='', ax=None):\n",
        "  if ax is None:\n",
        "    P.figure()\n",
        "  P.axis('off')\n",
        "\n",
        "  P.imshow(im, cmap=P.cm.gray, vmin=0, vmax=1)\n",
        "  P.title(title)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "def PreprocessImages(images):\n",
        "    # assumes input is 4-D, with range [0,255]\n",
        "    #\n",
        "    # torchvision have color channel as first dimension\n",
        "    # with normalization relative to mean/std of ImageNet:\n",
        "    #    https://pytorch.org/vision/stable/models.html\n",
        "    images = np.array(images)\n",
        "    images = images/255\n",
        "    images = np.transpose(images, (0,3,1,2))\n",
        "    images = torch.tensor(images, dtype=torch.float32)\n",
        "    images = transformer.forward(images)\n",
        "    return images.requires_grad_(True)"
      ],
      "metadata": {
        "id": "10o0c6Nxwvdn",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:38:31.417797500Z",
          "start_time": "2023-05-13T11:38:31.397293800Z"
        }
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualiseImageToHeatmap(image_3d, percentile=99):\n",
        "    r\"\"\"Returns a 3D tensor as RGB 3D heatmap\n",
        "    Pixels with higher weightage in sailiency heatmap will most saturated and will correspond to high RGB values in output heatmap_rgb\n",
        "  \"\"\"\n",
        "    image_2d = normaliseGradients(image_3d)\n",
        "    # Create heatmap using \"jet\" colormap, which returns an RGBA image\n",
        "    heatmap = plt.get_cmap('jet')(image_2d) * 255\n",
        "\n",
        "    # Normalise to 0,255 so it's visible when pasted\n",
        "    return Image.fromarray(heatmap.astype(np.uint8), mode='RGBA'), image_2d"
      ],
      "metadata": {
        "id": "1Eifbs32w2CP",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:38:31.447795700Z",
          "start_time": "2023-05-13T11:38:31.413796800Z"
        }
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LoadImage(file_path):\n",
        "    im = PIL.Image.open(file_path)\n",
        "    im = im.resize((299, 299))\n",
        "    im = np.asarray(im)\n",
        "    return im"
      ],
      "metadata": {
        "id": "bH5P4ljFw-GB",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:38:31.447795700Z",
          "start_time": "2023-05-13T11:38:31.427797900Z"
        }
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ShowImage(im, title='', ax=None):\n",
        "    if ax is None:\n",
        "        P.figure()\n",
        "    P.axis('off')\n",
        "    P.imshow(im)\n",
        "    P.title(title)"
      ],
      "metadata": {
        "id": "YXpx3NuHxHU7",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:38:31.466769200Z",
          "start_time": "2023-05-13T11:38:31.444798200Z"
        }
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global Variables"
      ],
      "metadata": {
        "collapsed": false,
        "id": "H9Z0wEw2c5Gb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "source": [
        "# This variable is used to globally set the size of the images used for training and viaulisation.\n",
        "# All images must be the same size for the techniques to work.\n",
        "image_size = (128, 128)\n",
        "lesion_segment_path = \"/content/HAM10000_segmentations_lesion_tschandl/\""
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.340720400Z",
          "start_time": "2023-05-13T11:26:40.310698700Z"
        },
        "id": "Ge0Mh2aUc5Gb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwaor_SUveCb"
      },
      "source": [
        "# Kaggle dataset loading\n",
        "If you're using your local machine, download the dataset into root/contents\n",
        "Kaggle doesn't have the segmentation images. Download this folder from the Harvard dataverse and place in /contents:\n",
        "\n",
        "https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T\n",
        "https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000\n",
        "\n",
        "You need a kaggle.json api key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Qp8DNrdxvi97",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.340720400Z",
          "start_time": "2023-05-13T11:26:40.327692200Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "9c890fae-092e-473f-a635-98f6b6e30a01"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-49090d3b-2e47-4433-801f-ac17614d6347\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-49090d3b-2e47-4433-801f-ac17614d6347\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Downloading skin-cancer-mnist-ham10000.zip to /content\n",
            "100% 5.20G/5.20G [04:29<00:00, 19.1MB/s]\n",
            "100% 5.20G/5.20G [04:29<00:00, 20.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d kmader/skin-cancer-mnist-ham10000\n",
        "!unzip -q skin-cancer-mnist-ham10000.zip -d content\n",
        "#Removing the zip to save space\n",
        "!rm skin-cancer-mnist-ham10000.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUH6AAsSJ6Eo",
        "outputId": "869fa04f-1646-495d-8dd3-6d5706103fc9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading segment images from Drive"
      ],
      "metadata": {
        "id": "ndlmh-OXSJbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/drive/MyDrive/HAM10000_segmentations_lesion_tschandl.zip -d content"
      ],
      "metadata": {
        "id": "7_ay8FiASP_D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c723eb94-85c3-4fc7-8095-a48909ff0cce"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/drive/MyDrive/HAM10000_segmentations_lesion_tschandl.zip, /content/drive/MyDrive/HAM10000_segmentations_lesion_tschandl.zip.zip or /content/drive/MyDrive/HAM10000_segmentations_lesion_tschandl.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek5Dfin9Svmd"
      },
      "source": [
        "## Dataset cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GEzN-8pAu8Bz",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.426306900Z",
          "start_time": "2023-05-13T11:26:40.341701Z"
        }
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('content/HAM10000_metadata.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "41stH92zu8B-",
        "outputId": "8eefb499-9cd6-4831-a099-b6477d0c6a19",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.426306900Z",
          "start_time": "2023-05-13T11:26:40.374694500Z"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               age\n",
              "count  9958.000000\n",
              "mean     51.863828\n",
              "std      16.968614\n",
              "min       0.000000\n",
              "25%      40.000000\n",
              "50%      50.000000\n",
              "75%      65.000000\n",
              "max      85.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-79859ccd-0778-43fd-9fc8-ffc67b77116d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>9958.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>51.863828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>16.968614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>40.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>50.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>65.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>85.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-79859ccd-0778-43fd-9fc8-ffc67b77116d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-79859ccd-0778-43fd-9fc8-ffc67b77116d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-79859ccd-0778-43fd-9fc8-ffc67b77116d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "U6CC22ABu8CB",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.426306900Z",
          "start_time": "2023-05-13T11:26:40.407278800Z"
        }
      },
      "outputs": [],
      "source": [
        "lesion_type_dict = {\n",
        "    'nv': 'Melanocytic nevi',\n",
        "    'mel': 'Melanoma',\n",
        "    'bkl': 'Bening keratosis-like lesions',\n",
        "    'bcc': 'Basal cell carcinoma',\n",
        "    'akiec': 'Actinic keratoses',\n",
        "    'vasc': 'Vascular lesions',\n",
        "    'df': 'Dermatofibroma'\n",
        "}\n",
        "ds_dir = 'content/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2-i_Kyq9u8CC",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.608819700Z",
          "start_time": "2023-05-13T11:26:40.423278500Z"
        }
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "## Let's map the image_id with it's image path from part 1 and part 2 folders\n",
        "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x\n",
        "                     for x in glob(os.path.join(ds_dir, '*', '*.jpg'))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Ge6Zi_Plu8CK",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.609805900Z",
          "start_time": "2023-05-13T11:26:40.553799Z"
        }
      },
      "outputs": [],
      "source": [
        "df['path'] = df['image_id'].map(imageid_path_dict.get)\n",
        "df['cell_type'] = df['dx'].map(lesion_type_dict.get)\n",
        "df['cell_type_idx'] = pd.Categorical(df['cell_type']).codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0WLC4Dnu8CT",
        "outputId": "18f77de4-9d7c-4f97-deb7-4edbe01bd9d0",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.609805900Z",
          "start_time": "2023-05-13T11:26:40.554800200Z"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lesion_id         0\n",
              "image_id          0\n",
              "dx                0\n",
              "dx_type           0\n",
              "age              57\n",
              "sex               0\n",
              "localization      0\n",
              "path              0\n",
              "cell_type         0\n",
              "cell_type_idx     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "source": [
        "# Fill in the null values with the average age\n",
        "df['age'].fillna((df['age'].mean()), inplace=True)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.610827500Z",
          "start_time": "2023-05-13T11:26:40.567798900Z"
        },
        "id": "dnQwzdIyc5Gc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lesion_id        0\n",
              "image_id         0\n",
              "dx               0\n",
              "dx_type          0\n",
              "age              0\n",
              "sex              0\n",
              "localization     0\n",
              "path             0\n",
              "cell_type        0\n",
              "cell_type_idx    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Check our dataset is cleaned for null values\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.610827500Z",
          "start_time": "2023-05-13T11:26:40.579805900Z"
        },
        "id": "CUokdrbGc5Gc",
        "outputId": "0d4ea707-d239-45ca-b6cb-e09072b94d1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JElKkmMj_8j"
      },
      "source": [
        "# Smoothgrad\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wiuvG8mOsNqk",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.610827500Z",
          "start_time": "2023-05-13T11:26:40.595801700Z"
        }
      },
      "outputs": [],
      "source": [
        "class_idx_str = 'class_idx_str'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "N0FkwjCJs8_n",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.670089300Z",
          "start_time": "2023-05-13T11:26:40.612800700Z"
        }
      },
      "outputs": [],
      "source": [
        "INPUT_OUTPUT_GRADIENTS = 'INPUT_OUTPUT_GRADIENTS'\n",
        "CONVOLUTION_LAYER_VALUES = 'CONVOLUTION_LAYER_VALUES'\n",
        "CONVOLUTION_OUTPUT_GRADIENTS = 'CONVOLUTION_OUTPUT_GRADIENTS'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Sa6perPzzK4E",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.757088500Z",
          "start_time": "2023-05-13T11:26:40.629089900Z"
        }
      },
      "outputs": [],
      "source": [
        "expected_keys = [INPUT_OUTPUT_GRADIENTS]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "xMKDQNRNsekV",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.757088500Z",
          "start_time": "2023-05-13T11:26:40.645089100Z"
        }
      },
      "outputs": [],
      "source": [
        "conv_layer_outputs = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "_XQ7SBxWqcSd",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.758087800Z",
          "start_time": "2023-05-13T11:26:40.666089900Z"
        }
      },
      "outputs": [],
      "source": [
        "def call_model_function(images, call_model_args=None, expected_keys=None):\n",
        "    target_class_idx =  call_model_args[class_idx_str]\n",
        "    images = tf.convert_to_tensor(images)\n",
        "    with tf.GradientTape() as tape:\n",
        "        if expected_keys==[INPUT_OUTPUT_GRADIENTS]:\n",
        "            tape.watch(images)\n",
        "            _, output_layer = new_model(images)\n",
        "            output_layer = output_layer[:,target_class_idx]\n",
        "            gradients = np.array(tape.gradient(output_layer, images))\n",
        "            return {INPUT_OUTPUT_GRADIENTS: gradients}\n",
        "        else:\n",
        "            conv_layer, output_layer = new_model(images)\n",
        "            gradients = np.array(tape.gradient(output_layer, conv_layer))\n",
        "            return {CONVOLUTION_LAYER_VALUES: conv_layer,\n",
        "                    CONVOLUTION_OUTPUT_GRADIENTS: gradients}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "6mbaFPIGOiYU",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.758087800Z",
          "start_time": "2023-05-13T11:26:40.696091100Z"
        }
      },
      "outputs": [],
      "source": [
        "def format_and_check_call_model_output(self, output, input_shape, expected_keys):\n",
        "  \"\"\"Converts keys in the output into an np.ndarray, and confirms its shape.\n",
        "\n",
        "  Args:\n",
        "    output: The output dictionary of data to be formatted.\n",
        "    input_shape: The shape of the input that yielded the output\n",
        "    expected_keys: List of keys inside output to format/check for shape agreement.\n",
        "\n",
        "  Raises:\n",
        "      ValueError: If output shapes do not match expected shape.\"\"\"\n",
        "  # If key is in check_full_shape, the shape should be equal to the input shape (e.g. \n",
        "  # INPUT_OUTPUT_GRADIENTS, which gives gradients for each value of the input). Otherwise,\n",
        "  # only checks the outermost dimension of output to match input_shape (i.e. the batch size\n",
        "  # should be the same).\n",
        "  check_full_shape = [INPUT_OUTPUT_GRADIENTS]\n",
        "  for expected_key in expected_keys:\n",
        "    output[expected_key] = np.asarray(output[expected_key])\n",
        "    expected_shape = input_shape\n",
        "    actual_shape = output[expected_key].shape\n",
        "    if expected_key not in check_full_shape:\n",
        "      expected_shape = expected_shape[0]\n",
        "      actual_shape = actual_shape[0]\n",
        "    if expected_shape != actual_shape:\n",
        "      raise ValueError(SHAPE_ERROR_MESSAGE[expected_key].format(\n",
        "                      expected_shape, actual_shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "xzMDaanyQOa4",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.759087600Z",
          "start_time": "2023-05-13T11:26:40.715092300Z"
        }
      },
      "outputs": [],
      "source": [
        "# Output of the last convolution layer for the given input, including the batch\n",
        "# dimension.\n",
        "CONVOLUTION_LAYER_VALUES = 'CONVOLUTION_LAYER_VALUES'\n",
        "# Gradients of the output being explained (the logit/softmax value) with respect\n",
        "# to the last convolution layer, including the batch dimension.\n",
        "CONVOLUTION_OUTPUT_GRADIENTS = 'CONVOLUTION_OUTPUT_GRADIENTS'\n",
        "# Gradients of the output being explained (the logit/softmax value) with respect\n",
        "# to the input. Shape should be the same shape as x_value_batch.\n",
        "INPUT_OUTPUT_GRADIENTS = 'INPUT_OUTPUT_GRADIENTS'\n",
        "# Value of the output being explained (the logit/softmax value).\n",
        "OUTPUT_LAYER_VALUES = 'OUTPUT_LAYER_VALUES'\n",
        "\n",
        "SHAPE_ERROR_MESSAGE = {\n",
        "    CONVOLUTION_LAYER_VALUES: (\n",
        "        'Expected outermost dimension of CONVOLUTION_LAYER_VALUES to be the '\n",
        "        'same as x_value_batch - expected {}, actual {}'\n",
        "    ),\n",
        "    CONVOLUTION_OUTPUT_GRADIENTS: (\n",
        "        'Expected outermost dimension of CONVOLUTION_OUTPUT_GRADIENTS to be the '\n",
        "        'same as x_value_batch - expected {}, actual {}'\n",
        "    ),\n",
        "    INPUT_OUTPUT_GRADIENTS: (\n",
        "        'Expected key INPUT_OUTPUT_GRADIENTS to be the same shape as input '\n",
        "        'x_value_batch - expected {}, actual {}'\n",
        "    ),\n",
        "    OUTPUT_LAYER_VALUES: (\n",
        "        'Expected outermost dimension of OUTPUT_LAYER_VALUES to be the same as'\n",
        "        ' x_value_batch - expected {}, actual {}'\n",
        "    ),\n",
        "}\n",
        "\n",
        "\n",
        "class CoreGradients(object):\n",
        "\n",
        "  def GetMask(self, x_value, call_model_function, call_model_args=None):\n",
        "    \"\"\"Returns an unsmoothed mask.\n",
        "\n",
        "    Args:\n",
        "      x_value: Input ndarray.\n",
        "      call_model_function: A function that interfaces with a model to return\n",
        "        specific output in a dictionary when given an input and other arguments.\n",
        "        Expected function signature:\n",
        "        - call_model_function(x_value_batch,\n",
        "                              call_model_args=None,\n",
        "                              expected_keys=None):\n",
        "          x_value_batch - Input for the model, given as a batch (i.e. dimension\n",
        "            0 is the batch dimension, dimensions 1 through n represent a single\n",
        "            input).\n",
        "          call_model_args - Other arguments used to call and run the model.\n",
        "          expected_keys - List of keys that are expected in the output. Possible\n",
        "            keys in this list are CONVOLUTION_LAYER_VALUES, \n",
        "            CONVOLUTION_OUTPUT_GRADIENTS, INPUT_OUTPUT_GRADIENTS, and\n",
        "            OUTPUT_LAYER_VALUES, and are explained in detail where declared.\n",
        "      call_model_args: The arguments that will be passed to the call model\n",
        "        function, for every call of the model.\n",
        "\n",
        "    \"\"\"\n",
        "    raise NotImplementedError('A derived class should implemented GetMask()')\n",
        "\n",
        "  def GetSmoothedMask(self,\n",
        "                      x_value,\n",
        "                      call_model_function,\n",
        "                      call_model_args=None,\n",
        "                      stdev_spread=.15,\n",
        "                      nsamples=25,\n",
        "                      magnitude=True,\n",
        "                      **kwargs):\n",
        "    \"\"\"Returns a mask that is smoothed with the SmoothGrad method.\n",
        "\n",
        "    Args:\n",
        "      x_value: Input ndarray.\n",
        "      call_model_function: A function that interfaces with a model to return\n",
        "        specific output in a dictionary when given an input and other arguments.\n",
        "        Expected function signature:\n",
        "        - call_model_function(x_value_batch,\n",
        "                              call_model_args=None,\n",
        "                              expected_keys=None):\n",
        "          x_value_batch - Input for the model, given as a batch (i.e. dimension\n",
        "            0 is the batch dimension, dimensions 1 through n represent a single\n",
        "            input).\n",
        "          call_model_args - Other arguments used to call and run the model.\n",
        "          expected_keys - List of keys that are expected in the output. Possible\n",
        "            keys in this list are CONVOLUTION_LAYER_VALUES,\n",
        "            CONVOLUTION_OUTPUT_GRADIENTS, INPUT_OUTPUT_GRADIENTS, and\n",
        "            OUTPUT_LAYER_VALUES, and are explained in detail where declared.\n",
        "      call_model_args: The arguments that will be passed to the call model\n",
        "        function, for every call of the model.\n",
        "      stdev_spread: Amount of noise to add to the input, as fraction of the\n",
        "                    total spread (x_max - x_min). Defaults to 15%.\n",
        "      nsamples: Number of samples to average across to get the smooth gradient.\n",
        "      magnitude: If true, computes the sum of squares of gradients instead of\n",
        "                 just the sum. Defaults to true.\n",
        "    \"\"\"\n",
        "    stdev = stdev_spread * (np.max(x_value) - np.min(x_value))\n",
        "    # Starting baseline image\n",
        "    total_gradients = np.zeros_like(x_value, dtype=np.float32)\n",
        "    for _ in range(nsamples):\n",
        "      noise = np.random.normal(0, stdev, x_value.shape)\n",
        "      # Calculate and add our smoothgrad noise\n",
        "      x_plus_noise = x_value + noise\n",
        "      # Get vanilla gradients. The input is the interpolated image + the Smoothgrad noise we generated\n",
        "      grad = self.GetMask(x_plus_noise, call_model_function, call_model_args,\n",
        "                          **kwargs)\n",
        "      if magnitude:\n",
        "        total_gradients += (grad * grad)\n",
        "      else:\n",
        "        total_gradients += grad\n",
        "\n",
        "    return total_gradients / nsamples\n",
        "\n",
        "  def format_and_check_call_model_output(self, output, input_shape, expected_keys):\n",
        "    \"\"\"Converts keys in the output into an np.ndarray, and confirms its shape.\n",
        "\n",
        "    Args:\n",
        "      output: The output dictionary of data to be formatted.\n",
        "      input_shape: The shape of the input that yielded the output\n",
        "      expected_keys: List of keys inside output to format/check for shape agreement.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If output shapes do not match expected shape.\"\"\"\n",
        "    # If key is in check_full_shape, the shape should be equal to the input shape (e.g. \n",
        "    # INPUT_OUTPUT_GRADIENTS, which gives gradients for each value of the input). Otherwise,\n",
        "    # only checks the outermost dimension of output to match input_shape (i.e. the batch size\n",
        "    # should be the same).\n",
        "    check_full_shape = [INPUT_OUTPUT_GRADIENTS]\n",
        "    for expected_key in expected_keys:\n",
        "      output[expected_key] = np.asarray(output[expected_key])\n",
        "      expected_shape = input_shape\n",
        "      actual_shape = output[expected_key].shape\n",
        "      if expected_key not in check_full_shape:\n",
        "        expected_shape = expected_shape[0]\n",
        "        actual_shape = actual_shape[0]\n",
        "      if expected_shape != actual_shape:\n",
        "        raise ValueError(SHAPE_ERROR_MESSAGE[expected_key].format(\n",
        "                       expected_shape, actual_shape))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "vMYQuC0onOFp",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.759087600Z",
          "start_time": "2023-05-13T11:26:40.741091100Z"
        }
      },
      "outputs": [],
      "source": [
        "# Inherits our smoothgrad technique in CoreGradients\n",
        "class Gradients(CoreGradients):\n",
        "\n",
        "  expected_keys = [INPUT_OUTPUT_GRADIENTS]\n",
        "\n",
        "  def GetMask(self, x_value, call_model_function, call_model_args=None):\n",
        "    \"\"\"Returns a vanilla gradients mask.\n",
        "\n",
        "    Args:\n",
        "      x_value: Input ndarray.\n",
        "      call_model_function: A function that interfaces with a model to return\n",
        "        specific data in a dictionary when given an input and other arguments.\n",
        "        Expected function signature:\n",
        "        - call_model_function(x_value_batch,\n",
        "                              call_model_args=None,\n",
        "                              expected_keys=None):\n",
        "          x_value_batch - Input for the model, given as a batch (i.e. dimension\n",
        "            0 is the batch dimension, dimensions 1 through n represent a single\n",
        "            input).\n",
        "          call_model_args - Other arguments used to call and run the model.\n",
        "          expected_keys - List of keys that are expected in the output. For this\n",
        "            method (Gradients), the expected keys are\n",
        "            INPUT_OUTPUT_GRADIENTS - Gradients of the output layer\n",
        "              (logit/softmax) with respect to the input. Shape should be the\n",
        "              same shape as x_value_batch.\n",
        "      call_model_args: The arguments that will be passed to the call model\n",
        "        function, for every call of the model.\n",
        "    \"\"\"\n",
        "    x_value_batched = np.expand_dims(x_value, axis=0)\n",
        "    call_model_output = call_model_function(\n",
        "        x_value_batched,\n",
        "        call_model_args=call_model_args,\n",
        "        expected_keys=self.expected_keys)\n",
        "\n",
        "    # Check gradient calculation is correct\n",
        "    self.format_and_check_call_model_output(call_model_output,\n",
        "                                            x_value_batched.shape,\n",
        "                                            self.expected_keys)\n",
        "\n",
        "    return call_model_output[INPUT_OUTPUT_GRADIENTS][0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "collapsed": false,
        "id": "1HXYqVgzc5Ge"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "source": [
        "# Apply the function to each image path in the 'path' column of the dataframe\n",
        "df['image'] = df['path'].apply(lambda x: read_and_resize_image(x, image_size))"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:28:13.089639800Z",
          "start_time": "2023-05-13T11:26:40.753092100Z"
        },
        "id": "pSuK_BQRc5Ge"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 128, 3)    10015\n",
              "Name: image, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "df['image'].map(lambda x: x.shape).value_counts()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:28:13.108590700Z",
          "start_time": "2023-05-13T11:28:13.091589100Z"
        },
        "id": "mWewaFiJc5Ge",
        "outputId": "9ffa7930-e44c-4ddb-9621-d64e6e1068d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [],
      "source": [
        "# Split the data into train and test sets\n",
        "# We use stratify which splits the dataset with the same class inbalance as the dataset.\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['cell_type_idx'], random_state=42)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:28:13.166254600Z",
          "start_time": "2023-05-13T11:28:13.107600600Z"
        },
        "id": "Nmw8J2v3c5Ge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the image data and target labels for train and test sets\n",
        "X_train = np.stack(train_df['image'].values)\n",
        "y_train = train_df['cell_type_idx'].values\n",
        "X_test = np.stack(test_df['image'].values)\n",
        "y_test = test_df['cell_type_idx'].values"
      ],
      "metadata": {
        "id": "7BlXbH7ymvDV"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16705208/16705208 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Compute class weights\n",
        "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "# Create a data generator for data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        zoom_range=0.15,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.15,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode=\"nearest\")\n",
        "\n",
        "# Convert the labels to one-hot encoding for use with categorical_crossentropy\n",
        "num_classes = df['cell_type_idx'].nunique()\n",
        "y_train_norm = to_categorical(y_train, num_classes=num_classes)\n",
        "y_test_norm = to_categorical(y_test, num_classes=num_classes)\n",
        "\n",
        "# Normalizing the input data to match the format the model was trained on\n",
        "X_train_norm = X_train / 255.0\n",
        "X_test_norm = X_test / 255.0\n",
        "\n",
        "# Load the pre-trained EfficientNetB0 model\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(image_size[0], image_size[1], 3))\n",
        "\n",
        "# Add a custom head for classification\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(rate=0.2)(x)\n",
        "x = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(x)  # Added L2 regularization\n",
        "predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "# This is the model we will train\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze all layers, keeping Batch Normalization layers in inference mode\n",
        "for layer in base_model.layers:\n",
        "    if not isinstance(layer, tensorflow.keras.layers.BatchNormalization):\n",
        "        layer.trainable = False\n",
        "\n",
        "# Unfreeze the last few layers of the base model\n",
        "for layer in base_model.layers[-5:]:\n",
        "    layer.trainable = True  \n",
        "\n",
        "# Learning Rate Schedule\n",
        "lr_schedule = tensorflow.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.2, patience=5, min_lr=0.0010)\n",
        "\n",
        "# Early Stopping\n",
        "early_stopping = tensorflow.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=10, restore_best_weights=True)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:28:13.259251500Z",
          "start_time": "2023-05-13T11:28:13.139250200Z"
        },
        "id": "neRTUzAMc5Ge",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42a3974d-158f-4d3d-8b83-029646ab8ce7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:28:13.526888300Z",
          "start_time": "2023-05-13T11:28:13.512858800Z"
        },
        "id": "gnthod6Vc5Gf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "125/125 [==============================] - 59s 215ms/step - loss: 5.4705 - accuracy: 0.4293 - val_loss: 1.8691 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "125/125 [==============================] - 28s 220ms/step - loss: 2.3827 - accuracy: 0.5331 - val_loss: 0.6865 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "125/125 [==============================] - 28s 220ms/step - loss: 1.6532 - accuracy: 0.5516 - val_loss: 0.3653 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "125/125 [==============================] - 27s 218ms/step - loss: 1.4088 - accuracy: 0.5757 - val_loss: 0.2633 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "125/125 [==============================] - 27s 219ms/step - loss: 1.2934 - accuracy: 0.5824 - val_loss: 0.2363 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "125/125 [==============================] - 27s 219ms/step - loss: 1.1916 - accuracy: 0.6047 - val_loss: 0.2166 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "125/125 [==============================] - 27s 219ms/step - loss: 1.1788 - accuracy: 0.6149 - val_loss: 0.2099 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "125/125 [==============================] - 28s 220ms/step - loss: 1.1426 - accuracy: 0.6086 - val_loss: 0.1964 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "125/125 [==============================] - 27s 219ms/step - loss: 1.0902 - accuracy: 0.6363 - val_loss: 0.1911 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "125/125 [==============================] - 28s 220ms/step - loss: 1.0333 - accuracy: 0.6438 - val_loss: 0.1857 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "125/125 [==============================] - 27s 217ms/step - loss: 1.0661 - accuracy: 0.6441 - val_loss: 0.2156 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "125/125 [==============================] - 27s 218ms/step - loss: 1.0846 - accuracy: 0.6273 - val_loss: 0.2472 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "125/125 [==============================] - 27s 218ms/step - loss: 1.0170 - accuracy: 0.6476 - val_loss: 0.1880 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "125/125 [==============================] - 27s 219ms/step - loss: 0.9658 - accuracy: 0.6530 - val_loss: 0.1904 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "125/125 [==============================] - 27s 219ms/step - loss: 0.9792 - accuracy: 0.6631 - val_loss: 0.1743 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "125/125 [==============================] - 27s 218ms/step - loss: 0.9403 - accuracy: 0.6754 - val_loss: 0.1708 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "125/125 [==============================] - 27s 218ms/step - loss: 0.9119 - accuracy: 0.6539 - val_loss: 0.1557 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "125/125 [==============================] - 27s 218ms/step - loss: 0.8620 - accuracy: 0.6812 - val_loss: 0.1621 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "125/125 [==============================] - 27s 218ms/step - loss: 0.8657 - accuracy: 0.6831 - val_loss: 0.1625 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 20/50\n",
            "125/125 [==============================] - 28s 220ms/step - loss: 0.8634 - accuracy: 0.6892 - val_loss: 0.1518 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 21/50\n",
            "125/125 [==============================] - 27s 219ms/step - loss: 0.8620 - accuracy: 0.6731 - val_loss: 0.1748 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 22/50\n",
            "125/125 [==============================] - 27s 217ms/step - loss: 0.8236 - accuracy: 0.6847 - val_loss: 0.1568 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 23/50\n",
            "125/125 [==============================] - 27s 219ms/step - loss: 0.8711 - accuracy: 0.6841 - val_loss: 0.1707 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 24/50\n",
            "125/125 [==============================] - 27s 218ms/step - loss: 0.8322 - accuracy: 0.6844 - val_loss: 0.1654 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 25/50\n",
            "125/125 [==============================] - 27s 219ms/step - loss: 0.7922 - accuracy: 0.7055 - val_loss: 0.1475 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 26/50\n",
            "125/125 [==============================] - 27s 219ms/step - loss: 0.7404 - accuracy: 0.7087 - val_loss: 0.1465 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 27/50\n",
            "125/125 [==============================] - 27s 219ms/step - loss: 0.7389 - accuracy: 0.7055 - val_loss: 0.1496 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 28/50\n",
            "125/125 [==============================] - 27s 218ms/step - loss: 0.7414 - accuracy: 0.7032 - val_loss: 0.1475 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 29/50\n",
            "125/125 [==============================] - 27s 220ms/step - loss: 0.7708 - accuracy: 0.7124 - val_loss: 0.1427 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 30/50\n",
            "125/125 [==============================] - 27s 220ms/step - loss: 0.6953 - accuracy: 0.7177 - val_loss: 0.1328 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 31/50\n",
            "125/125 [==============================] - 27s 218ms/step - loss: 0.7233 - accuracy: 0.7105 - val_loss: 0.1472 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 32/50\n",
            "125/125 [==============================] - 27s 217ms/step - loss: 0.7174 - accuracy: 0.7124 - val_loss: 0.1350 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 33/50\n",
            "125/125 [==============================] - 27s 219ms/step - loss: 0.6722 - accuracy: 0.7297 - val_loss: 0.1246 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 34/50\n",
            "125/125 [==============================] - 27s 218ms/step - loss: 0.6590 - accuracy: 0.7304 - val_loss: 0.1284 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 35/50\n",
            "125/125 [==============================] - 27s 217ms/step - loss: 0.6861 - accuracy: 0.7265 - val_loss: 0.1363 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 36/50\n",
            "125/125 [==============================] - 27s 219ms/step - loss: 0.6957 - accuracy: 0.7139 - val_loss: 0.1374 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 37/50\n",
            "125/125 [==============================] - 27s 217ms/step - loss: 0.6672 - accuracy: 0.7255 - val_loss: 0.1391 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 38/50\n",
            "125/125 [==============================] - 28s 221ms/step - loss: 0.6438 - accuracy: 0.7388 - val_loss: 0.1582 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 39/50\n",
            "125/125 [==============================] - 29s 228ms/step - loss: 0.6721 - accuracy: 0.7340 - val_loss: 0.1398 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 40/50\n",
            "125/125 [==============================] - 28s 227ms/step - loss: 0.6525 - accuracy: 0.7348 - val_loss: 0.1407 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 41/50\n",
            "125/125 [==============================] - 29s 231ms/step - loss: 0.6224 - accuracy: 0.7396 - val_loss: 0.1281 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 42/50\n",
            "125/125 [==============================] - 29s 228ms/step - loss: 0.5693 - accuracy: 0.7472 - val_loss: 0.1123 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 43/50\n",
            "125/125 [==============================] - 29s 229ms/step - loss: 0.6043 - accuracy: 0.7457 - val_loss: 0.1309 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 44/50\n",
            "125/125 [==============================] - 29s 229ms/step - loss: 0.5737 - accuracy: 0.7609 - val_loss: 0.1120 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 45/50\n",
            "125/125 [==============================] - 29s 228ms/step - loss: 0.5598 - accuracy: 0.7579 - val_loss: 0.1114 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 46/50\n",
            "125/125 [==============================] - 29s 229ms/step - loss: 0.5903 - accuracy: 0.7505 - val_loss: 0.1279 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 47/50\n",
            "125/125 [==============================] - 29s 228ms/step - loss: 0.5765 - accuracy: 0.7552 - val_loss: 0.1565 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 48/50\n",
            "125/125 [==============================] - 29s 228ms/step - loss: 0.6733 - accuracy: 0.7529 - val_loss: 0.1909 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 49/50\n",
            "125/125 [==============================] - 29s 228ms/step - loss: 0.6097 - accuracy: 0.7495 - val_loss: 0.1376 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 50/50\n",
            "125/125 [==============================] - 29s 229ms/step - loss: 0.5587 - accuracy: 0.7614 - val_loss: 0.1224 - val_accuracy: 0.0000e+00 - lr: 0.0010\n"
          ]
        }
      ],
      "source": [
        "#Train the model\n",
        "history = model.fit(train_datagen.flow(X_train_norm, y_train_norm, batch_size=64), \n",
        "          validation_data=(X_test_norm,  ), \n",
        "          class_weight=class_weights, \n",
        "          steps_per_epoch=len(X_train_norm) // 64,\n",
        "          epochs=50,\n",
        "          callbacks=[lr_schedule, early_stopping])"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:19.600734300Z",
          "start_time": "2023-05-13T11:28:13.528858800Z"
        },
        "id": "g4zcnIzoc5Gf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b7f3278-f843-4c56-c1ca-15be4cecf99f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metric ouputs"
      ],
      "metadata": {
        "collapsed": false,
        "id": "krdgxQIec5Gf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 2s 11ms/step\n",
            "F1 score: 0.74\n",
            "Precision: 0.80\n",
            "Recall: 0.71\n",
            "Accuracy: 0.71\n",
            "63/63 [==============================] - 1s 10ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "output_metrics(model, X_test_norm, y_test_norm)\n",
        "plot_confusion_matrix(model, X_test_norm, y_test_norm)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:21.441120300Z",
          "start_time": "2023-05-13T11:30:19.578739100Z"
        },
        "id": "_tO-X_9xc5Gf",
        "outputId": "8f366888-526c-4176-a676-a8c178860da5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 805
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "outputs": [],
      "source": [
        "# Create a new DataFrame with 'lesion_id' and 'cell_type_dx' columns from the test data\n",
        "result_df = pd.DataFrame({'lesion_id': test_df['lesion_id'], 'target': y_test})"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:21.456089300Z",
          "start_time": "2023-05-13T11:30:21.444087800Z"
        },
        "id": "iSI4DKTIc5Gf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualisation Application"
      ],
      "metadata": {
        "id": "nXV_484h8qBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "heatmap_images = []\n",
        "raw_gradients = []"
      ],
      "metadata": {
        "id": "SazY4Jp63LlD",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:21.492122900Z",
          "start_time": "2023-05-13T11:30:21.459081700Z"
        }
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the index of the last convolutional layer\n",
        "last_conv_index = None\n",
        "for i, layer in enumerate(model.layers[::-1]):\n",
        "    if 'conv' in layer.name:\n",
        "        last_conv_index = len(model.layers) - 1 - i\n",
        "        break\n",
        "\n",
        "if last_conv_index is not None:\n",
        "    # Select the last convolutional layer\n",
        "    last_conv_layer = model.get_layer(index=last_conv_index)\n",
        "\n",
        "    # Create a new model with the last convolutional layer as output\n",
        "    new_model = tf.keras.models.Model(inputs=model.input, outputs=[last_conv_layer.output, model.output])\n",
        "\n",
        "    # Print information about the selected layer\n",
        "    print(\"Selected layer name:\", last_conv_layer.name)\n",
        "    print(\"Selected layer output shape:\", last_conv_layer.output_shape)\n",
        "else:\n",
        "    print(\"No convolutional layer found in the model.\")"
      ],
      "metadata": {
        "id": "hggXkmPO3DYK",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:21.863501900Z",
          "start_time": "2023-05-13T11:30:21.474091900Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28f82ae9-0b0d-472a-efe7-d08f00ff6d6c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected layer name: top_conv\n",
            "Selected layer output shape: (None, 4, 4, 1280)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smoothgrad = Gradients()"
      ],
      "metadata": {
        "id": "_nfiPssg9rfe",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:21.898501700Z",
          "start_time": "2023-05-13T11:30:21.863501900Z"
        }
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = df['path']"
      ],
      "metadata": {
        "id": "XWJbnZ2dKKQP",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:21.961502800Z",
          "start_time": "2023-05-13T11:30:21.871498700Z"
        }
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 2s 9ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(X_test)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:24.282176900Z",
          "start_time": "2023-05-13T11:30:21.919500300Z"
        },
        "id": "1IM4aEync5Gg",
        "outputId": "061f0d87-66cd-46c4-cde7-6a8995ecd13f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "outputs": [],
      "source": [
        "baseline = np.zeros(image_size)\n",
        "prediction_class = np.argmax(predictions[0])\n",
        "call_model_args = {class_idx_str: prediction_class}"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:24.431252Z",
          "start_time": "2023-05-13T11:30:24.284176800Z"
        },
        "id": "XxuyX0iOc5Gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty DataFrame to store the collected samples\n",
        "collected_samples = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "# Iterate over each unique class value\n",
        "for class_value in range(5):\n",
        "    # Filter the DataFrame to select samples with the current class value\n",
        "    class_samples = df[df['cell_type_idx'] == class_value].sample(n=100, random_state=42)\n",
        "    \n",
        "    # Append the selected samples to the collected_samples DataFrame\n",
        "    collected_samples = pd.concat([collected_samples, class_samples], ignore_index=True)"
      ],
      "metadata": {
        "id": "DdhB8yBeAtOp"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "images = collected_samples['path']\n",
        "\n",
        "progress_bar = tqdm(total=len(images))\n",
        "\n",
        "for img in images:\n",
        "  img_arr = read_and_resize_image(image_path=img, size=image_size)\n",
        "  im_tensor = PreprocessImages([img_arr])\n",
        "  im = img_arr.astype(np.float32)\n",
        "  vanilla_integrated_gradients_mask_3d = smoothgrad.GetSmoothedMask(\n",
        "    im, call_model_function, call_model_args)\n",
        "  raw_gradients.append(vanilla_integrated_gradients_mask_3d)\n",
        "  \n",
        "  # Update the progress bar\n",
        "  progress_bar.update(1)\n",
        "\n",
        "# Close the progress bar\n",
        "progress_bar.close()"
      ],
      "metadata": {
        "id": "fe1WZKB92AuA",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:34:13.383166700Z",
          "start_time": "2023-05-13T11:33:24.147667800Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "958242d7-20da-4706-8cac-95b0d571d3b7"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [1:12:48<00:00,  8.74s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "segment_image_list = load_images_from_folder(\"/content/content/HAM10000_segmentations_lesion_tschandl\")"
      ],
      "metadata": {
        "id": "hm-D3L80RVoN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "949b249f-5693-4c65-bccb-b5476afbdbff"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-1ff13f1d996d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msegment_image_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images_from_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/content/HAM10000_segmentations_lesion_tschandl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-d087f19d9c9a>\u001b[0m in \u001b[0;36mload_images_from_folder\u001b[0;34m(folder_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_images_from_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mimage_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/content/HAM10000_segmentations_lesion_tschandl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "boolean_masks = []\n",
        "for img in segment_image_list:\n",
        "    boolean_masks.append(convert_to_boolean_mask(img))"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T12:31:46.207680600Z",
          "start_time": "2023-05-13T12:31:34.524780200Z"
        },
        "id": "yj5_y0_Jc5Gg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "overlap = []\n",
        "for i in range(len(images)):\n",
        "    per = [calculate_overlap(raw_gradients[i][:, :, 0], boolean_masks[i]), np.argmax(predictions, axis=1)[i]]\n",
        "    if str(per[0]) != \"nan\":\n",
        "      overlap.append(per)\n"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T12:33:03.713782Z",
          "start_time": "2023-05-13T12:33:03.671023500Z"
        },
        "id": "DbjKVuGVc5Gg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Calculate average overlap for each class\n",
        "class_overlaps = defaultdict(list)\n",
        "for item in overlap:\n",
        "    overlap_value = item[0]\n",
        "    class_label = item[1]\n",
        "    class_overlaps[class_label].append(overlap_value)\n",
        "\n",
        "class_labels = []\n",
        "average_overlaps = []\n",
        "\n",
        "for class_label, overlaps in class_overlaps.items():\n",
        "    class_labels.append(class_label)\n",
        "    average_overlap = sum(overlaps) / len(overlaps)\n",
        "    average_overlaps.append(average_overlap)\n",
        "\n",
        "# Create a bar chart or scatter plot\n",
        "plt.figure(figsize=(10, 6))  # Adjust the figure size as per your preference\n",
        "\n",
        "mapped_labels = [list(lesion_type_dict.values())[label] for label in class_labels]\n",
        "\n",
        "plt.figure(figsize=(20, 6))\n",
        "# Bar chart\n",
        "plt.bar(range(len(average_overlaps)), average_overlaps)\n",
        "plt.xticks(range(len(average_overlaps)), mapped_labels)\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Average Overlap')\n",
        "plt.title('Average Overlap of Gradients over Masks')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T12:34:59.508194500Z",
          "start_time": "2023-05-13T12:34:48.572324700Z"
        },
        "id": "biHVKI-Uc5Gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for label, overlap in zip(mapped_labels, average_overlaps):\n",
        "    print(f\"Label: {label}, Average Overlap: {overlap}\")"
      ],
      "metadata": {
        "id": "09ZgOYyG6VVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Plotting the loss graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plotting the accuracy graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tOVxcIGqc5Gh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP"
      ],
      "metadata": {
        "id": "M_KhKcFZ_G73"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "outputs": [],
      "source": [
        "EffNet = model\n",
        "from keras.layers import Input, concatenate\n",
        "\n",
        "def perceptron():\n",
        "  mlp = Sequential()\n",
        "  mlp.add(Input(shape=(3,)))\n",
        "  mlp.add(Flatten())\n",
        "  mlp.add(Dense(128, activation='relu'))\n",
        "  mlp.add(Dropout(0.5))\n",
        "  mlp.add(Dense(64, activation='relu'))\n",
        "  mlp.add(Dense(len(df['cell_type_idx'].unique()), activation='softmax'))\n",
        "  return mlp"
      ],
      "metadata": {
        "id": "OWw3_1ZypsCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron = perceptron()\n",
        "perceptron.compile(optimizer = 'adam', loss= 'sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "duVgHNta_KxY"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sex_dict = {\n",
        "    'male': 0.0,\n",
        "    'female': 1.0,\n",
        "    'unknown': 1.5\n",
        "}\n",
        "\n",
        "loc_dict = {\n",
        "    'back': 1.0,\n",
        "    'lower extremity': 2.0,\n",
        "    'trunk': 3.0,\n",
        "    'upper extremity': 4.0,\n",
        "    'abdomen': 5.0,\n",
        "    'face': 6.0,\n",
        "    'chest': 7.0,\n",
        "    'foot': 8.0,\n",
        "    'unknown': 9.0,\n",
        "    'neck': 10.0,\n",
        "    'scalp': 11.0,\n",
        "    'hand': 12.0,\n",
        "    'ear': 13.0,\n",
        "    'genital': 14.0,\n",
        "    'acral': 15.0\n",
        "}\n",
        "\n",
        "train_df = train_df.replace({\"sex\": sex_dict})\n",
        "train_df = train_df.replace({\"localization\": loc_dict})\n",
        "test_df = test_df.replace({\"sex\": sex_dict})\n",
        "test_df = test_df.replace({\"localization\": loc_dict})\n",
        "\n",
        "#Extract and recombine the demographic data for training set and test set\n",
        "X_train_demo = np.stack((np.asarray(train_df['age'].values), np.asarray(train_df['sex'].values), np.asarray(train_df['localization'].values)))\n",
        "y_train = train_df['cell_type_idx'].values\n",
        "X_test_demo = np.stack((np.asarray(test_df['age'].values), np.asarray(test_df['sex'].values), np.asarray(test_df['localization'].values)))\n",
        "y_test = test_df['cell_type_idx'].values\n",
        "\n",
        "X_train_demo=X_train_demo.T.astype(int)\n",
        "X_test_demo=X_test_demo.T.astype(int)"
      ],
      "metadata": {
        "id": "s7mjulC9ASEi"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron.fit(X_train_demo, y_train, batch_size=256, epochs=50, validation_split=0.3)"
      ],
      "metadata": {
        "id": "ffiWyIHVAVDC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "844eb952-09f6-4c0e-9338-e0b61dfbc899"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "22/22 [==============================] - 2s 13ms/step - loss: 6.5761 - accuracy: 0.3820 - val_loss: 4.4054 - val_accuracy: 0.6710\n",
            "Epoch 2/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 3.1006 - accuracy: 0.4470 - val_loss: 2.3082 - val_accuracy: 0.6710\n",
            "Epoch 3/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.4226 - accuracy: 0.4977 - val_loss: 1.7455 - val_accuracy: 0.6710\n",
            "Epoch 4/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 2.0931 - accuracy: 0.5159 - val_loss: 1.5253 - val_accuracy: 0.6710\n",
            "Epoch 5/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.9039 - accuracy: 0.5209 - val_loss: 1.4165 - val_accuracy: 0.6710\n",
            "Epoch 6/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.7442 - accuracy: 0.5414 - val_loss: 1.3891 - val_accuracy: 0.6710\n",
            "Epoch 7/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.6798 - accuracy: 0.5544 - val_loss: 1.3343 - val_accuracy: 0.6710\n",
            "Epoch 8/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.5833 - accuracy: 0.5817 - val_loss: 1.3206 - val_accuracy: 0.6710\n",
            "Epoch 9/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.5161 - accuracy: 0.5954 - val_loss: 1.2967 - val_accuracy: 0.6710\n",
            "Epoch 10/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.4671 - accuracy: 0.5990 - val_loss: 1.2689 - val_accuracy: 0.6710\n",
            "Epoch 11/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.4277 - accuracy: 0.6170 - val_loss: 1.2560 - val_accuracy: 0.6710\n",
            "Epoch 12/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.3915 - accuracy: 0.6255 - val_loss: 1.2367 - val_accuracy: 0.6710\n",
            "Epoch 13/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.3687 - accuracy: 0.6295 - val_loss: 1.2318 - val_accuracy: 0.6710\n",
            "Epoch 14/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.3476 - accuracy: 0.6393 - val_loss: 1.2238 - val_accuracy: 0.6710\n",
            "Epoch 15/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.3225 - accuracy: 0.6355 - val_loss: 1.2149 - val_accuracy: 0.6710\n",
            "Epoch 16/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.3018 - accuracy: 0.6464 - val_loss: 1.2011 - val_accuracy: 0.6710\n",
            "Epoch 17/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.2804 - accuracy: 0.6558 - val_loss: 1.1924 - val_accuracy: 0.6710\n",
            "Epoch 18/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.2561 - accuracy: 0.6521 - val_loss: 1.1865 - val_accuracy: 0.6710\n",
            "Epoch 19/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.2501 - accuracy: 0.6594 - val_loss: 1.1630 - val_accuracy: 0.6710\n",
            "Epoch 20/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.2133 - accuracy: 0.6587 - val_loss: 1.1471 - val_accuracy: 0.6710\n",
            "Epoch 21/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.2107 - accuracy: 0.6612 - val_loss: 1.1413 - val_accuracy: 0.6710\n",
            "Epoch 22/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.1935 - accuracy: 0.6639 - val_loss: 1.1390 - val_accuracy: 0.6710\n",
            "Epoch 23/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1855 - accuracy: 0.6653 - val_loss: 1.1304 - val_accuracy: 0.6710\n",
            "Epoch 24/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1771 - accuracy: 0.6641 - val_loss: 1.1101 - val_accuracy: 0.6710\n",
            "Epoch 25/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1669 - accuracy: 0.6658 - val_loss: 1.1199 - val_accuracy: 0.6705\n",
            "Epoch 26/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1536 - accuracy: 0.6665 - val_loss: 1.1059 - val_accuracy: 0.6705\n",
            "Epoch 27/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1533 - accuracy: 0.6639 - val_loss: 1.0857 - val_accuracy: 0.6710\n",
            "Epoch 28/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.1356 - accuracy: 0.6673 - val_loss: 1.0789 - val_accuracy: 0.6710\n",
            "Epoch 29/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1240 - accuracy: 0.6673 - val_loss: 1.0716 - val_accuracy: 0.6710\n",
            "Epoch 30/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1204 - accuracy: 0.6665 - val_loss: 1.0616 - val_accuracy: 0.6710\n",
            "Epoch 31/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.1196 - accuracy: 0.6657 - val_loss: 1.0506 - val_accuracy: 0.6710\n",
            "Epoch 32/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.1085 - accuracy: 0.6676 - val_loss: 1.0441 - val_accuracy: 0.6710\n",
            "Epoch 33/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0953 - accuracy: 0.6674 - val_loss: 1.0360 - val_accuracy: 0.6705\n",
            "Epoch 34/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0906 - accuracy: 0.6660 - val_loss: 1.0212 - val_accuracy: 0.6710\n",
            "Epoch 35/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0897 - accuracy: 0.6664 - val_loss: 1.0289 - val_accuracy: 0.6710\n",
            "Epoch 36/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0743 - accuracy: 0.6689 - val_loss: 1.0144 - val_accuracy: 0.6710\n",
            "Epoch 37/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0793 - accuracy: 0.6690 - val_loss: 1.0193 - val_accuracy: 0.6710\n",
            "Epoch 38/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0698 - accuracy: 0.6694 - val_loss: 1.0138 - val_accuracy: 0.6705\n",
            "Epoch 39/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.0573 - accuracy: 0.6689 - val_loss: 1.0091 - val_accuracy: 0.6747\n",
            "Epoch 40/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0575 - accuracy: 0.6696 - val_loss: 1.0027 - val_accuracy: 0.6710\n",
            "Epoch 41/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0602 - accuracy: 0.6717 - val_loss: 0.9962 - val_accuracy: 0.6710\n",
            "Epoch 42/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0496 - accuracy: 0.6698 - val_loss: 1.0042 - val_accuracy: 0.6747\n",
            "Epoch 43/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0614 - accuracy: 0.6683 - val_loss: 1.0013 - val_accuracy: 0.6710\n",
            "Epoch 44/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0474 - accuracy: 0.6708 - val_loss: 0.9939 - val_accuracy: 0.6722\n",
            "Epoch 45/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0455 - accuracy: 0.6694 - val_loss: 0.9907 - val_accuracy: 0.6705\n",
            "Epoch 46/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0499 - accuracy: 0.6685 - val_loss: 1.0040 - val_accuracy: 0.6714\n",
            "Epoch 47/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0435 - accuracy: 0.6733 - val_loss: 0.9919 - val_accuracy: 0.6705\n",
            "Epoch 48/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0464 - accuracy: 0.6726 - val_loss: 0.9874 - val_accuracy: 0.6705\n",
            "Epoch 49/50\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 1.0379 - accuracy: 0.6710 - val_loss: 0.9858 - val_accuracy: 0.6714\n",
            "Epoch 50/50\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 1.0363 - accuracy: 0.6703 - val_loss: 0.9834 - val_accuracy: 0.6722\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa8f81ace50>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the trained model to make predictions on the test data\n",
        "#output_metrics(perceptron, X_test_demo, y_test)"
      ],
      "metadata": {
        "id": "rrQIs7KeAY7a"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concatenated_inputs = concatenate(inputs=[perceptron.output, EffNet.output])\n",
        "x = Dense(128, activation=\"relu\")(concatenated_inputs)\n",
        "x = Dense(64, activation=\"relu\")(x)\n",
        "x = Dense(len(df['cell_type_idx'].unique()), activation='softmax')(x)\n",
        "model = Model(inputs=[perceptron.input, EffNet.input], outputs=x)\n",
        "#plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Q5m8FslyAZVU"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([X_train_demo, X_train], y_train, batch_size=256, epochs=30, validation_split=0.3)"
      ],
      "metadata": {
        "id": "0IOiBAbEAj4C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d8a621-b525-46cd-b217-08ceb7049ac0"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "22/22 [==============================] - 29s 211ms/step - loss: 1.6772 - accuracy: 0.5792 - val_loss: 1.3682 - val_accuracy: 0.6710\n",
            "Epoch 2/30\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 1.0469 - accuracy: 0.6689 - val_loss: 1.0728 - val_accuracy: 0.6710\n",
            "Epoch 3/30\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.7910 - accuracy: 0.7366 - val_loss: 0.9537 - val_accuracy: 0.6926\n",
            "Epoch 4/30\n",
            "22/22 [==============================] - 3s 134ms/step - loss: 0.6515 - accuracy: 0.8055 - val_loss: 0.9103 - val_accuracy: 0.6968\n",
            "Epoch 5/30\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.5534 - accuracy: 0.8340 - val_loss: 0.9976 - val_accuracy: 0.6968\n",
            "Epoch 6/30\n",
            "22/22 [==============================] - 3s 134ms/step - loss: 0.4721 - accuracy: 0.8675 - val_loss: 0.8411 - val_accuracy: 0.7587\n",
            "Epoch 7/30\n",
            "22/22 [==============================] - 3s 134ms/step - loss: 0.4027 - accuracy: 0.8923 - val_loss: 0.8011 - val_accuracy: 0.7700\n",
            "Epoch 8/30\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.3431 - accuracy: 0.9107 - val_loss: 0.7516 - val_accuracy: 0.7987\n",
            "Epoch 9/30\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.3194 - accuracy: 0.9146 - val_loss: 0.7682 - val_accuracy: 0.7779\n",
            "Epoch 10/30\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.2697 - accuracy: 0.9326 - val_loss: 0.7059 - val_accuracy: 0.8082\n",
            "Epoch 11/30\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.2423 - accuracy: 0.9433 - val_loss: 0.7679 - val_accuracy: 0.8082\n",
            "Epoch 12/30\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.2290 - accuracy: 0.9478 - val_loss: 0.7055 - val_accuracy: 0.8145\n",
            "Epoch 13/30\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.1890 - accuracy: 0.9608 - val_loss: 0.6691 - val_accuracy: 0.8136\n",
            "Epoch 14/30\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.1706 - accuracy: 0.9636 - val_loss: 0.7260 - val_accuracy: 0.8195\n",
            "Epoch 15/30\n",
            "22/22 [==============================] - 3s 134ms/step - loss: 0.1744 - accuracy: 0.9659 - val_loss: 0.7177 - val_accuracy: 0.8132\n",
            "Epoch 16/30\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.1557 - accuracy: 0.9699 - val_loss: 0.7937 - val_accuracy: 0.8274\n",
            "Epoch 17/30\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.1476 - accuracy: 0.9716 - val_loss: 0.7470 - val_accuracy: 0.8145\n",
            "Epoch 18/30\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.1410 - accuracy: 0.9733 - val_loss: 0.7519 - val_accuracy: 0.8182\n",
            "Epoch 19/30\n",
            "22/22 [==============================] - 3s 134ms/step - loss: 0.1358 - accuracy: 0.9729 - val_loss: 0.7682 - val_accuracy: 0.8265\n",
            "Epoch 20/30\n",
            "22/22 [==============================] - 3s 134ms/step - loss: 0.1233 - accuracy: 0.9781 - val_loss: 0.7771 - val_accuracy: 0.8224\n",
            "Epoch 21/30\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.1157 - accuracy: 0.9795 - val_loss: 0.7832 - val_accuracy: 0.8203\n",
            "Epoch 22/30\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.1131 - accuracy: 0.9795 - val_loss: 0.8187 - val_accuracy: 0.8191\n",
            "Epoch 23/30\n",
            "22/22 [==============================] - 3s 134ms/step - loss: 0.1139 - accuracy: 0.9820 - val_loss: 0.8131 - val_accuracy: 0.8161\n",
            "Epoch 24/30\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.1110 - accuracy: 0.9818 - val_loss: 0.9125 - val_accuracy: 0.8170\n",
            "Epoch 25/30\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.1064 - accuracy: 0.9820 - val_loss: 0.7908 - val_accuracy: 0.8307\n",
            "Epoch 26/30\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.0948 - accuracy: 0.9843 - val_loss: 0.8021 - val_accuracy: 0.8182\n",
            "Epoch 27/30\n",
            "22/22 [==============================] - 3s 134ms/step - loss: 0.0900 - accuracy: 0.9843 - val_loss: 0.7778 - val_accuracy: 0.8278\n",
            "Epoch 28/30\n",
            "22/22 [==============================] - 3s 134ms/step - loss: 0.0833 - accuracy: 0.9852 - val_loss: 0.8456 - val_accuracy: 0.8224\n",
            "Epoch 29/30\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.0801 - accuracy: 0.9873 - val_loss: 0.8205 - val_accuracy: 0.8174\n",
            "Epoch 30/30\n",
            "22/22 [==============================] - 3s 133ms/step - loss: 0.0873 - accuracy: 0.9870 - val_loss: 0.8333 - val_accuracy: 0.8215\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa8f813f820>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_metrics(model, [X_test_demo, X_test], y_test)"
      ],
      "metadata": {
        "id": "72yUPebpApIf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "67e26956-3d2d-4e3f-cd8d-643603fbc669"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-b50692430b3d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX_test_demo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_img\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_test_img' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}