{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toby-j/COM3025-CW/blob/main/Eff_net_gradients_with_Demographics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgNdhpLOvFjm"
      },
      "source": [
        "# Imports\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "qDkEGwmWu8Bx",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.119677700Z",
          "start_time": "2023-05-13T11:26:33.840438300Z"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "from keras import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import PIL.Image\n",
        "from torchvision import models, transforms\n",
        "from matplotlib import pylab as P\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorflow\n",
        "from keras.applications import EfficientNetB0\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "from keras.regularizers import l2\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7A6B6RebmmE"
      },
      "source": [
        "\n",
        "# Global functions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "collapsed": false,
        "id": "Ga5fFxodc5GZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss(history):\n",
        "  # Plotting the loss graph\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(history.history['loss'], label='Training Loss')\n",
        "  plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  # Plotting the accuracy graph\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "  plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "  plt.title('Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "cixTYO4qYoad"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(model, X_test, y_test):\n",
        "  y_pred = model.predict(X_test)\n",
        "  y_pred_classes = np.argmax(y_pred, axis=-1)\n",
        "  \n",
        "  confusion_matrix = metrics.confusion_matrix(y_test, y_pred_classes)\n",
        "\n",
        "  labels = list(set(y_test) | set(y_pred_classes))\n",
        "  cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=labels)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(10, 8))\n",
        "  cm_display.plot(ax=ax)\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.154694900Z",
          "start_time": "2023-05-13T11:26:40.123676300Z"
        },
        "id": "xnjaiTnGc5GZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7jxu5s9dbp9P",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.175693500Z",
          "start_time": "2023-05-13T11:26:40.139694100Z"
        }
      },
      "outputs": [],
      "source": [
        "def output_metrics(model, data, labels):\n",
        "  predictions = model.predict(data)\n",
        "\n",
        "  y_pred  = np.argmax(predictions, axis=-1)\n",
        "\n",
        "  print(f'Accuracy score: {accuracy_score(labels, y_pred)}')\n",
        "  print(f'F1 score: {f1_score(labels, y_pred, average=\"weighted\")}')\n",
        "  print(f'Precision score: {precision_score(labels, y_pred, average=\"weighted\")}')\n",
        "  print(f'Recall score: {recall_score(labels, y_pred, average=\"weighted\")}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing"
      ],
      "metadata": {
        "collapsed": false,
        "id": "8DmNjb8xc5GZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "source": [
        "def load_images_from_folder(folder_path):\n",
        "    image_paths = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        img_path = os.path.join(folder_path, filename)\n",
        "        if os.path.isfile(img_path):\n",
        "            image_paths.append(img_path)\n",
        "    return image_paths"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.176728100Z",
          "start_time": "2023-05-13T11:26:40.155692700Z"
        },
        "id": "cWYuYQz-c5GZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "source": [
        "def read_and_resize_image(image_path, size):\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.resize(img, size)\n",
        "    return img"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.203697200Z",
          "start_time": "2023-05-13T11:26:40.168692900Z"
        },
        "id": "EsiNHATJc5GZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "source": [
        "def normaliseGradients(image_3d, percentile=99):\n",
        "    image_2d = np.sum(np.abs(image_3d), axis=2)\n",
        "\n",
        "    # Get max pixel value in the image\n",
        "    vmax = np.percentile(image_2d, percentile)\n",
        "    # Get minimum pixel value in the image\n",
        "    vmin = np.min(image_2d)\n",
        "\n",
        "    # Normalise the values. We clip intensities so values lower than 0 are equal 0.\n",
        "    return np.clip((image_2d - vmin) / (vmax - vmin), 0, 1)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.253719400Z",
          "start_time": "2023-05-13T11:26:40.186692800Z"
        },
        "id": "qatwwSwVc5Ga"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "source": [
        "def convert_to_boolean_mask(image):\n",
        "    # Convert the image to a NumPy array\n",
        "    image = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Create a boolean mask where white pixels are True and black pixels are False\n",
        "    binary_image = np.where(image == 255, False, True)  # Assuming white pixels are represented as 255\n",
        "\n",
        "    cropped_image = cv2.resize(binary_image.astype(np.uint8), image_size)\n",
        "\n",
        "    return cropped_image"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.253719400Z",
          "start_time": "2023-05-13T11:26:40.201698Z"
        },
        "id": "5kRkd7Zvc5Ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculations"
      ],
      "metadata": {
        "collapsed": false,
        "id": "R04qroeIc5Ga"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "source": [
        "def calculate_overlap(g, m):\n",
        "    # Invert the mask, so the pixels outside are True.\n",
        "    # Replace where the mask is False, with a 0 in the same location in raw_gradients\n",
        "    segment = np.where(np.array(m), np.array(g), 0)\n",
        "\n",
        "    # We now have just the gradients in a 2D vector of the pixels outside the bounding box\n",
        "    sum_mask_segment = np.sum(segment)\n",
        "    # Find what percentage the outside pixels make up of the full gradient image by summing both 2D vectors\n",
        "    total_sum = np.sum(g)\n",
        "    # What percentage are the gradients outside the segment of the full gradient vector\n",
        "    overlap = (sum_mask_segment / total_sum) * 100\n",
        "\n",
        "    return overlap"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T12:32:24.830687900Z",
          "start_time": "2023-05-13T12:32:24.775654200Z"
        },
        "id": "Aghw834qc5Ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilities"
      ],
      "metadata": {
        "collapsed": false,
        "id": "3wlHWquEc5Ga"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fGgdn5RUfZ-V",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:38:31.397293800Z",
          "start_time": "2023-05-13T11:38:31.383282900Z"
        }
      },
      "outputs": [],
      "source": [
        "def ShowGrayscaleImage(im, title='', ax=None):\n",
        "  if ax is None:\n",
        "    P.figure()\n",
        "  P.axis('off')\n",
        "\n",
        "  P.imshow(im, cmap=P.cm.gray, vmin=0, vmax=1)\n",
        "  P.title(title)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "def PreprocessImages(images):\n",
        "    # assumes input is 4-D, with range [0,255]\n",
        "    #\n",
        "    # torchvision have color channel as first dimension\n",
        "    # with normalization relative to mean/std of ImageNet:\n",
        "    #    https://pytorch.org/vision/stable/models.html\n",
        "    images = np.array(images)\n",
        "    images = images/255\n",
        "    images = np.transpose(images, (0,3,1,2))\n",
        "    images = torch.tensor(images, dtype=torch.float32)\n",
        "    images = transformer.forward(images)\n",
        "    return images.requires_grad_(True)"
      ],
      "metadata": {
        "id": "10o0c6Nxwvdn",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:38:31.417797500Z",
          "start_time": "2023-05-13T11:38:31.397293800Z"
        }
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualiseImageToHeatmap(image_3d, percentile=99):\n",
        "    r\"\"\"Returns a 3D tensor as RGB 3D heatmap\n",
        "    Pixels with higher weightage in sailiency heatmap will most saturated and will correspond to high RGB values in output heatmap_rgb\n",
        "  \"\"\"\n",
        "    image_2d = normaliseGradients(image_3d)\n",
        "    # Create heatmap using \"jet\" colormap, which returns an RGBA image\n",
        "    heatmap = plt.get_cmap('jet')(image_2d) * 255\n",
        "\n",
        "    # Normalise to 0,255 so it's visible when pasted\n",
        "    return Image.fromarray(heatmap.astype(np.uint8), mode='RGBA'), image_2d"
      ],
      "metadata": {
        "id": "1Eifbs32w2CP",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:38:31.447795700Z",
          "start_time": "2023-05-13T11:38:31.413796800Z"
        }
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LoadImage(file_path):\n",
        "    im = PIL.Image.open(file_path)\n",
        "    im = im.resize((299, 299))\n",
        "    im = np.asarray(im)\n",
        "    return im"
      ],
      "metadata": {
        "id": "bH5P4ljFw-GB",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:38:31.447795700Z",
          "start_time": "2023-05-13T11:38:31.427797900Z"
        }
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ShowImage(im, title='', ax=None):\n",
        "    if ax is None:\n",
        "        P.figure()\n",
        "    P.axis('off')\n",
        "    P.imshow(im)\n",
        "    P.title(title)"
      ],
      "metadata": {
        "id": "YXpx3NuHxHU7",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:38:31.466769200Z",
          "start_time": "2023-05-13T11:38:31.444798200Z"
        }
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Global Variables"
      ],
      "metadata": {
        "collapsed": false,
        "id": "H9Z0wEw2c5Gb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "source": [
        "# This variable is used to globally set the size of the images used for training and viaulisation.\n",
        "# All images must be the same size for the techniques to work.\n",
        "image_size = (128, 128)\n",
        "lesion_segment_path = \"/content/HAM10000_segmentations_lesion_tschandl/\""
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.340720400Z",
          "start_time": "2023-05-13T11:26:40.310698700Z"
        },
        "id": "Ge0Mh2aUc5Gb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwaor_SUveCb"
      },
      "source": [
        "# Kaggle dataset loading\n",
        "If you're using your local machine, download the dataset into root/contents\n",
        "Kaggle doesn't have the segmentation images. Download this folder from the Harvard dataverse and place in /contents:\n",
        "\n",
        "https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T\n",
        "https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000\n",
        "\n",
        "You need a kaggle.json api key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Qp8DNrdxvi97",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.340720400Z",
          "start_time": "2023-05-13T11:26:40.327692200Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "84d6c205-b694-49e1-cf2c-b22a0ad20720"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-86e4464c-eb2e-441e-b92a-506df3facb7f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-86e4464c-eb2e-441e-b92a-506df3facb7f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Downloading skin-cancer-mnist-ham10000.zip to /content\n",
            "100% 5.18G/5.20G [00:26<00:00, 242MB/s]\n",
            "100% 5.20G/5.20G [00:27<00:00, 206MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d kmader/skin-cancer-mnist-ham10000\n",
        "!unzip -q skin-cancer-mnist-ham10000.zip -d content\n",
        "#Removing the zip to save space\n",
        "!rm skin-cancer-mnist-ham10000.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CUH6AAsSJ6Eo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dc900c4-dc39-4611-fd9a-9164a596f6fd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading segment images from Drive"
      ],
      "metadata": {
        "id": "ndlmh-OXSJbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/drive/MyDrive/HAM10000_segmentations_lesion_tschandl.zip -d content"
      ],
      "metadata": {
        "id": "7_ay8FiASP_D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef705a1c-e7a4-4f03-8ae7-76cc98bf1c48"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/drive/MyDrive/HAM10000_segmentations_lesion_tschandl.zip, /content/drive/MyDrive/HAM10000_segmentations_lesion_tschandl.zip.zip or /content/drive/MyDrive/HAM10000_segmentations_lesion_tschandl.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek5Dfin9Svmd"
      },
      "source": [
        "## Dataset cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GEzN-8pAu8Bz",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.426306900Z",
          "start_time": "2023-05-13T11:26:40.341701Z"
        }
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('content/HAM10000_metadata.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "41stH92zu8B-",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.426306900Z",
          "start_time": "2023-05-13T11:26:40.374694500Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "84a8470d-51f3-449f-d17c-b16f2a1de45d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               age\n",
              "count  9958.000000\n",
              "mean     51.863828\n",
              "std      16.968614\n",
              "min       0.000000\n",
              "25%      40.000000\n",
              "50%      50.000000\n",
              "75%      65.000000\n",
              "max      85.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ca785c9b-8017-4aaf-8486-789dc47e6548\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>9958.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>51.863828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>16.968614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>40.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>50.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>65.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>85.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ca785c9b-8017-4aaf-8486-789dc47e6548')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ca785c9b-8017-4aaf-8486-789dc47e6548 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ca785c9b-8017-4aaf-8486-789dc47e6548');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "U6CC22ABu8CB",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.426306900Z",
          "start_time": "2023-05-13T11:26:40.407278800Z"
        }
      },
      "outputs": [],
      "source": [
        "lesion_type_dict = {\n",
        "    'nv': 'Melanocytic nevi',\n",
        "    'mel': 'Melanoma',\n",
        "    'bkl': 'Bening keratosis-like lesions',\n",
        "    'bcc': 'Basal cell carcinoma',\n",
        "    'akiec': 'Actinic keratoses',\n",
        "    'vasc': 'Vascular lesions',\n",
        "    'df': 'Dermatofibroma'\n",
        "}\n",
        "ds_dir = 'content/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2-i_Kyq9u8CC",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.608819700Z",
          "start_time": "2023-05-13T11:26:40.423278500Z"
        }
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "## Let's map the image_id with it's image path from part 1 and part 2 folders\n",
        "imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x\n",
        "                     for x in glob(os.path.join(ds_dir, '*', '*.jpg'))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Ge6Zi_Plu8CK",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.609805900Z",
          "start_time": "2023-05-13T11:26:40.553799Z"
        }
      },
      "outputs": [],
      "source": [
        "df['path'] = df['image_id'].map(imageid_path_dict.get)\n",
        "df['cell_type'] = df['dx'].map(lesion_type_dict.get)\n",
        "df['cell_type_idx'] = pd.Categorical(df['cell_type']).codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "e0WLC4Dnu8CT",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.609805900Z",
          "start_time": "2023-05-13T11:26:40.554800200Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc4b495f-b393-4881-a042-c9f5155f3c05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lesion_id         0\n",
              "image_id          0\n",
              "dx                0\n",
              "dx_type           0\n",
              "age              57\n",
              "sex               0\n",
              "localization      0\n",
              "path              0\n",
              "cell_type         0\n",
              "cell_type_idx     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "source": [
        "# Fill in the null values with the average age\n",
        "df['age'].fillna((df['age'].mean()), inplace=True)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.610827500Z",
          "start_time": "2023-05-13T11:26:40.567798900Z"
        },
        "id": "dnQwzdIyc5Gc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lesion_id        0\n",
              "image_id         0\n",
              "dx               0\n",
              "dx_type          0\n",
              "age              0\n",
              "sex              0\n",
              "localization     0\n",
              "path             0\n",
              "cell_type        0\n",
              "cell_type_idx    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# Check our dataset is cleaned for null values\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.610827500Z",
          "start_time": "2023-05-13T11:26:40.579805900Z"
        },
        "id": "CUokdrbGc5Gc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de4ec08-57d4-4366-fcae-e6ebb0666510"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JElKkmMj_8j"
      },
      "source": [
        "# Smoothgrad\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "wiuvG8mOsNqk",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.610827500Z",
          "start_time": "2023-05-13T11:26:40.595801700Z"
        }
      },
      "outputs": [],
      "source": [
        "class_idx_str = 'class_idx_str'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "N0FkwjCJs8_n",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.670089300Z",
          "start_time": "2023-05-13T11:26:40.612800700Z"
        }
      },
      "outputs": [],
      "source": [
        "INPUT_OUTPUT_GRADIENTS = 'INPUT_OUTPUT_GRADIENTS'\n",
        "CONVOLUTION_LAYER_VALUES = 'CONVOLUTION_LAYER_VALUES'\n",
        "CONVOLUTION_OUTPUT_GRADIENTS = 'CONVOLUTION_OUTPUT_GRADIENTS'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Sa6perPzzK4E",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.757088500Z",
          "start_time": "2023-05-13T11:26:40.629089900Z"
        }
      },
      "outputs": [],
      "source": [
        "expected_keys = [INPUT_OUTPUT_GRADIENTS]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "xMKDQNRNsekV",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.757088500Z",
          "start_time": "2023-05-13T11:26:40.645089100Z"
        }
      },
      "outputs": [],
      "source": [
        "conv_layer_outputs = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_XQ7SBxWqcSd",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.758087800Z",
          "start_time": "2023-05-13T11:26:40.666089900Z"
        }
      },
      "outputs": [],
      "source": [
        "def call_model_function(images, call_model_args=None, expected_keys=None):\n",
        "    target_class_idx =  call_model_args[class_idx_str]\n",
        "    images = tf.convert_to_tensor(images)\n",
        "    with tf.GradientTape() as tape:\n",
        "        if expected_keys==[INPUT_OUTPUT_GRADIENTS]:\n",
        "            tape.watch(images)\n",
        "            _, output_layer = new_model(images)\n",
        "            output_layer = output_layer[:,target_class_idx]\n",
        "            gradients = np.array(tape.gradient(output_layer, images))\n",
        "            return {INPUT_OUTPUT_GRADIENTS: gradients}\n",
        "        else:\n",
        "            conv_layer, output_layer = new_model(images)\n",
        "            gradients = np.array(tape.gradient(output_layer, conv_layer))\n",
        "            return {CONVOLUTION_LAYER_VALUES: conv_layer,\n",
        "                    CONVOLUTION_OUTPUT_GRADIENTS: gradients}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6mbaFPIGOiYU",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.758087800Z",
          "start_time": "2023-05-13T11:26:40.696091100Z"
        }
      },
      "outputs": [],
      "source": [
        "def format_and_check_call_model_output(self, output, input_shape, expected_keys):\n",
        "  \"\"\"Converts keys in the output into an np.ndarray, and confirms its shape.\n",
        "\n",
        "  Args:\n",
        "    output: The output dictionary of data to be formatted.\n",
        "    input_shape: The shape of the input that yielded the output\n",
        "    expected_keys: List of keys inside output to format/check for shape agreement.\n",
        "\n",
        "  Raises:\n",
        "      ValueError: If output shapes do not match expected shape.\"\"\"\n",
        "  # If key is in check_full_shape, the shape should be equal to the input shape (e.g. \n",
        "  # INPUT_OUTPUT_GRADIENTS, which gives gradients for each value of the input). Otherwise,\n",
        "  # only checks the outermost dimension of output to match input_shape (i.e. the batch size\n",
        "  # should be the same).\n",
        "  check_full_shape = [INPUT_OUTPUT_GRADIENTS]\n",
        "  for expected_key in expected_keys:\n",
        "    output[expected_key] = np.asarray(output[expected_key])\n",
        "    expected_shape = input_shape\n",
        "    actual_shape = output[expected_key].shape\n",
        "    if expected_key not in check_full_shape:\n",
        "      expected_shape = expected_shape[0]\n",
        "      actual_shape = actual_shape[0]\n",
        "    if expected_shape != actual_shape:\n",
        "      raise ValueError(SHAPE_ERROR_MESSAGE[expected_key].format(\n",
        "                      expected_shape, actual_shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "xzMDaanyQOa4",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.759087600Z",
          "start_time": "2023-05-13T11:26:40.715092300Z"
        }
      },
      "outputs": [],
      "source": [
        "# Output of the last convolution layer for the given input, including the batch\n",
        "# dimension.\n",
        "CONVOLUTION_LAYER_VALUES = 'CONVOLUTION_LAYER_VALUES'\n",
        "# Gradients of the output being explained (the logit/softmax value) with respect\n",
        "# to the last convolution layer, including the batch dimension.\n",
        "CONVOLUTION_OUTPUT_GRADIENTS = 'CONVOLUTION_OUTPUT_GRADIENTS'\n",
        "# Gradients of the output being explained (the logit/softmax value) with respect\n",
        "# to the input. Shape should be the same shape as x_value_batch.\n",
        "INPUT_OUTPUT_GRADIENTS = 'INPUT_OUTPUT_GRADIENTS'\n",
        "# Value of the output being explained (the logit/softmax value).\n",
        "OUTPUT_LAYER_VALUES = 'OUTPUT_LAYER_VALUES'\n",
        "\n",
        "SHAPE_ERROR_MESSAGE = {\n",
        "    CONVOLUTION_LAYER_VALUES: (\n",
        "        'Expected outermost dimension of CONVOLUTION_LAYER_VALUES to be the '\n",
        "        'same as x_value_batch - expected {}, actual {}'\n",
        "    ),\n",
        "    CONVOLUTION_OUTPUT_GRADIENTS: (\n",
        "        'Expected outermost dimension of CONVOLUTION_OUTPUT_GRADIENTS to be the '\n",
        "        'same as x_value_batch - expected {}, actual {}'\n",
        "    ),\n",
        "    INPUT_OUTPUT_GRADIENTS: (\n",
        "        'Expected key INPUT_OUTPUT_GRADIENTS to be the same shape as input '\n",
        "        'x_value_batch - expected {}, actual {}'\n",
        "    ),\n",
        "    OUTPUT_LAYER_VALUES: (\n",
        "        'Expected outermost dimension of OUTPUT_LAYER_VALUES to be the same as'\n",
        "        ' x_value_batch - expected {}, actual {}'\n",
        "    ),\n",
        "}\n",
        "\n",
        "\n",
        "class CoreGradients(object):\n",
        "\n",
        "  def GetMask(self, x_value, call_model_function, call_model_args=None):\n",
        "    \"\"\"Returns an unsmoothed mask.\n",
        "\n",
        "    Args:\n",
        "      x_value: Input ndarray.\n",
        "      call_model_function: A function that interfaces with a model to return\n",
        "        specific output in a dictionary when given an input and other arguments.\n",
        "        Expected function signature:\n",
        "        - call_model_function(x_value_batch,\n",
        "                              call_model_args=None,\n",
        "                              expected_keys=None):\n",
        "          x_value_batch - Input for the model, given as a batch (i.e. dimension\n",
        "            0 is the batch dimension, dimensions 1 through n represent a single\n",
        "            input).\n",
        "          call_model_args - Other arguments used to call and run the model.\n",
        "          expected_keys - List of keys that are expected in the output. Possible\n",
        "            keys in this list are CONVOLUTION_LAYER_VALUES, \n",
        "            CONVOLUTION_OUTPUT_GRADIENTS, INPUT_OUTPUT_GRADIENTS, and\n",
        "            OUTPUT_LAYER_VALUES, and are explained in detail where declared.\n",
        "      call_model_args: The arguments that will be passed to the call model\n",
        "        function, for every call of the model.\n",
        "\n",
        "    \"\"\"\n",
        "    raise NotImplementedError('A derived class should implemented GetMask()')\n",
        "\n",
        "  def GetSmoothedMask(self,\n",
        "                      x_value,\n",
        "                      call_model_function,\n",
        "                      call_model_args=None,\n",
        "                      stdev_spread=.15,\n",
        "                      nsamples=25,\n",
        "                      magnitude=True,\n",
        "                      **kwargs):\n",
        "    \"\"\"Returns a mask that is smoothed with the SmoothGrad method.\n",
        "\n",
        "    Args:\n",
        "      x_value: Input ndarray.\n",
        "      call_model_function: A function that interfaces with a model to return\n",
        "        specific output in a dictionary when given an input and other arguments.\n",
        "        Expected function signature:\n",
        "        - call_model_function(x_value_batch,\n",
        "                              call_model_args=None,\n",
        "                              expected_keys=None):\n",
        "          x_value_batch - Input for the model, given as a batch (i.e. dimension\n",
        "            0 is the batch dimension, dimensions 1 through n represent a single\n",
        "            input).\n",
        "          call_model_args - Other arguments used to call and run the model.\n",
        "          expected_keys - List of keys that are expected in the output. Possible\n",
        "            keys in this list are CONVOLUTION_LAYER_VALUES,\n",
        "            CONVOLUTION_OUTPUT_GRADIENTS, INPUT_OUTPUT_GRADIENTS, and\n",
        "            OUTPUT_LAYER_VALUES, and are explained in detail where declared.\n",
        "      call_model_args: The arguments that will be passed to the call model\n",
        "        function, for every call of the model.\n",
        "      stdev_spread: Amount of noise to add to the input, as fraction of the\n",
        "                    total spread (x_max - x_min). Defaults to 15%.\n",
        "      nsamples: Number of samples to average across to get the smooth gradient.\n",
        "      magnitude: If true, computes the sum of squares of gradients instead of\n",
        "                 just the sum. Defaults to true.\n",
        "    \"\"\"\n",
        "    stdev = stdev_spread * (np.max(x_value) - np.min(x_value))\n",
        "    # Starting baseline image\n",
        "    total_gradients = np.zeros_like(x_value, dtype=np.float32)\n",
        "    for _ in range(nsamples):\n",
        "      noise = np.random.normal(0, stdev, x_value.shape)\n",
        "      # Calculate and add our smoothgrad noise\n",
        "      x_plus_noise = x_value + noise\n",
        "      # Get vanilla gradients. The input is the interpolated image + the Smoothgrad noise we generated\n",
        "      grad = self.GetMask(x_plus_noise, call_model_function, call_model_args,\n",
        "                          **kwargs)\n",
        "      if magnitude:\n",
        "        total_gradients += (grad * grad)\n",
        "      else:\n",
        "        total_gradients += grad\n",
        "\n",
        "    return total_gradients / nsamples\n",
        "\n",
        "  def format_and_check_call_model_output(self, output, input_shape, expected_keys):\n",
        "    \"\"\"Converts keys in the output into an np.ndarray, and confirms its shape.\n",
        "\n",
        "    Args:\n",
        "      output: The output dictionary of data to be formatted.\n",
        "      input_shape: The shape of the input that yielded the output\n",
        "      expected_keys: List of keys inside output to format/check for shape agreement.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If output shapes do not match expected shape.\"\"\"\n",
        "    # If key is in check_full_shape, the shape should be equal to the input shape (e.g. \n",
        "    # INPUT_OUTPUT_GRADIENTS, which gives gradients for each value of the input). Otherwise,\n",
        "    # only checks the outermost dimension of output to match input_shape (i.e. the batch size\n",
        "    # should be the same).\n",
        "    check_full_shape = [INPUT_OUTPUT_GRADIENTS]\n",
        "    for expected_key in expected_keys:\n",
        "      output[expected_key] = np.asarray(output[expected_key])\n",
        "      expected_shape = input_shape\n",
        "      actual_shape = output[expected_key].shape\n",
        "      if expected_key not in check_full_shape:\n",
        "        expected_shape = expected_shape[0]\n",
        "        actual_shape = actual_shape[0]\n",
        "      if expected_shape != actual_shape:\n",
        "        raise ValueError(SHAPE_ERROR_MESSAGE[expected_key].format(\n",
        "                       expected_shape, actual_shape))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "vMYQuC0onOFp",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:26:40.759087600Z",
          "start_time": "2023-05-13T11:26:40.741091100Z"
        }
      },
      "outputs": [],
      "source": [
        "# Inherits our smoothgrad technique in CoreGradients\n",
        "class Gradients(CoreGradients):\n",
        "\n",
        "  expected_keys = [INPUT_OUTPUT_GRADIENTS]\n",
        "\n",
        "  def GetMask(self, x_value, call_model_function, call_model_args=None):\n",
        "    \"\"\"Returns a vanilla gradients mask.\n",
        "\n",
        "    Args:\n",
        "      x_value: Input ndarray.\n",
        "      call_model_function: A function that interfaces with a model to return\n",
        "        specific data in a dictionary when given an input and other arguments.\n",
        "        Expected function signature:\n",
        "        - call_model_function(x_value_batch,\n",
        "                              call_model_args=None,\n",
        "                              expected_keys=None):\n",
        "          x_value_batch - Input for the model, given as a batch (i.e. dimension\n",
        "            0 is the batch dimension, dimensions 1 through n represent a single\n",
        "            input).\n",
        "          call_model_args - Other arguments used to call and run the model.\n",
        "          expected_keys - List of keys that are expected in the output. For this\n",
        "            method (Gradients), the expected keys are\n",
        "            INPUT_OUTPUT_GRADIENTS - Gradients of the output layer\n",
        "              (logit/softmax) with respect to the input. Shape should be the\n",
        "              same shape as x_value_batch.\n",
        "      call_model_args: The arguments that will be passed to the call model\n",
        "        function, for every call of the model.\n",
        "    \"\"\"\n",
        "    x_value_batched = np.expand_dims(x_value, axis=0)\n",
        "    call_model_output = call_model_function(\n",
        "        x_value_batched,\n",
        "        call_model_args=call_model_args,\n",
        "        expected_keys=self.expected_keys)\n",
        "\n",
        "    # Check gradient calculation is correct\n",
        "    self.format_and_check_call_model_output(call_model_output,\n",
        "                                            x_value_batched.shape,\n",
        "                                            self.expected_keys)\n",
        "\n",
        "    return call_model_output[INPUT_OUTPUT_GRADIENTS][0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "collapsed": false,
        "id": "1HXYqVgzc5Ge"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "source": [
        "# Apply the function to each image path in the 'path' column of the dataframe\n",
        "df['image'] = df['path'].apply(lambda x: read_and_resize_image(x, image_size))"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:28:13.089639800Z",
          "start_time": "2023-05-13T11:26:40.753092100Z"
        },
        "id": "pSuK_BQRc5Ge"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128, 128, 3)    10015\n",
              "Name: image, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "df['image'].map(lambda x: x.shape).value_counts()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:28:13.108590700Z",
          "start_time": "2023-05-13T11:28:13.091589100Z"
        },
        "id": "mWewaFiJc5Ge",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0d6231b-4fd4-401d-9fae-97e5c43613c6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "outputs": [],
      "source": [
        "# Split the data into train and test sets\n",
        "# We use stratify which splits the dataset with the same class inbalance as the dataset.\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['cell_type_idx'], random_state=42)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:28:13.166254600Z",
          "start_time": "2023-05-13T11:28:13.107600600Z"
        },
        "id": "Nmw8J2v3c5Ge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the image data and target labels for train and test sets\n",
        "X_train = np.stack(train_df['image'].values)\n",
        "y_train = train_df['cell_type_idx'].values\n",
        "X_test = np.stack(test_df['image'].values)\n",
        "y_test = test_df['cell_type_idx'].values"
      ],
      "metadata": {
        "id": "7BlXbH7ymvDV"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16705208/16705208 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Compute class weights\n",
        "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "# Create a data generator for data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        zoom_range=0.15,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.15,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode=\"nearest\")\n",
        "\n",
        "# Convert the labels to one-hot encoding for use with categorical_crossentropy\n",
        "num_classes = df['cell_type_idx'].nunique()\n",
        "y_train_norm = to_categorical(y_train, num_classes=num_classes)\n",
        "y_test_norm = to_categorical(y_test, num_classes=num_classes)\n",
        "\n",
        "# Normalizing the input data to match the format the model was trained on\n",
        "X_train_norm = X_train / 255.0\n",
        "X_test_norm = X_test / 255.0\n",
        "\n",
        "# Load the pre-trained EfficientNetB0 model\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(image_size[0], image_size[1], 3))\n",
        "\n",
        "# Add a custom head for classification\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(rate=0.2)(x)\n",
        "x = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(x)  # Added L2 regularization\n",
        "predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "# This is the model we will train\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze all layers, keeping Batch Normalization layers in inference mode\n",
        "for layer in base_model.layers:\n",
        "    if not isinstance(layer, tensorflow.keras.layers.BatchNormalization):\n",
        "        layer.trainable = False\n",
        "\n",
        "# Unfreeze the last few layers of the base model\n",
        "for layer in base_model.layers[-5:]:\n",
        "    layer.trainable = True  \n",
        "\n",
        "# Learning Rate Schedule\n",
        "lr_schedule = tensorflow.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.2, patience=5, min_lr=0.0010)\n",
        "\n",
        "# Early Stopping\n",
        "early_stopping = tensorflow.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=10, restore_best_weights=True)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:28:13.259251500Z",
          "start_time": "2023-05-13T11:28:13.139250200Z"
        },
        "id": "neRTUzAMc5Ge",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faefcdf3-45a2-42af-eb5b-c94cb9fddeda"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:28:13.526888300Z",
          "start_time": "2023-05-13T11:28:13.512858800Z"
        },
        "id": "gnthod6Vc5Gf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "125/125 [==============================] - 59s 216ms/step - loss: 5.3954 - accuracy: 0.4576 - val_loss: 1.7698 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 2/5\n",
            "125/125 [==============================] - 27s 219ms/step - loss: 2.3275 - accuracy: 0.5338 - val_loss: 0.6195 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
            "Epoch 3/5\n",
            " 33/125 [======>.......................] - ETA: 20s - loss: 1.6451 - accuracy: 0.5687"
          ]
        }
      ],
      "source": [
        "#Train the model\n",
        "history = model.fit(train_datagen.flow(X_train_norm, y_train_norm, batch_size=64), \n",
        "          validation_data=(X_test_norm,  ), \n",
        "          class_weight=class_weights, \n",
        "          steps_per_epoch=len(X_train_norm) // 64,\n",
        "          epochs=5,\n",
        "          callbacks=[lr_schedule, early_stopping])"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:19.600734300Z",
          "start_time": "2023-05-13T11:28:13.528858800Z"
        },
        "id": "g4zcnIzoc5Gf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e26efec-48e9-4097-d1d6-8279561e2ddd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metric ouputs"
      ],
      "metadata": {
        "collapsed": false,
        "id": "krdgxQIec5Gf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "output_metrics(model, X_test_norm, y_test_norm)\n",
        "plot_confusion_matrix(model, X_test_norm, y_test_norm)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:21.441120300Z",
          "start_time": "2023-05-13T11:30:19.578739100Z"
        },
        "id": "_tO-X_9xc5Gf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Create a new DataFrame with 'lesion_id' and 'cell_type_dx' columns from the test data\n",
        "result_df = pd.DataFrame({'lesion_id': test_df['lesion_id'], 'target': y_test})"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:21.456089300Z",
          "start_time": "2023-05-13T11:30:21.444087800Z"
        },
        "id": "iSI4DKTIc5Gf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualisation Application"
      ],
      "metadata": {
        "id": "nXV_484h8qBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "heatmap_images = []\n",
        "raw_gradients = []"
      ],
      "metadata": {
        "id": "SazY4Jp63LlD",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:21.492122900Z",
          "start_time": "2023-05-13T11:30:21.459081700Z"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the index of the last convolutional layer\n",
        "last_conv_index = None\n",
        "for i, layer in enumerate(model.layers[::-1]):\n",
        "    if 'conv' in layer.name:\n",
        "        last_conv_index = len(model.layers) - 1 - i\n",
        "        break\n",
        "\n",
        "if last_conv_index is not None:\n",
        "    # Select the last convolutional layer\n",
        "    last_conv_layer = model.get_layer(index=last_conv_index)\n",
        "\n",
        "    # Create a new model with the last convolutional layer as output\n",
        "    new_model = tf.keras.models.Model(inputs=model.input, outputs=[last_conv_layer.output, model.output])\n",
        "\n",
        "    # Print information about the selected layer\n",
        "    print(\"Selected layer name:\", last_conv_layer.name)\n",
        "    print(\"Selected layer output shape:\", last_conv_layer.output_shape)\n",
        "else:\n",
        "    print(\"No convolutional layer found in the model.\")"
      ],
      "metadata": {
        "id": "hggXkmPO3DYK",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:21.863501900Z",
          "start_time": "2023-05-13T11:30:21.474091900Z"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smoothgrad = Gradients()"
      ],
      "metadata": {
        "id": "_nfiPssg9rfe",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:21.898501700Z",
          "start_time": "2023-05-13T11:30:21.863501900Z"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = df['path']"
      ],
      "metadata": {
        "id": "XWJbnZ2dKKQP",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:21.961502800Z",
          "start_time": "2023-05-13T11:30:21.871498700Z"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "predictions = model.predict(X_test)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:24.282176900Z",
          "start_time": "2023-05-13T11:30:21.919500300Z"
        },
        "id": "1IM4aEync5Gg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "baseline = np.zeros(image_size)\n",
        "prediction_class = np.argmax(predictions[0])\n",
        "call_model_args = {class_idx_str: prediction_class}"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T11:30:24.431252Z",
          "start_time": "2023-05-13T11:30:24.284176800Z"
        },
        "id": "XxuyX0iOc5Gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty DataFrame to store the collected samples\n",
        "collected_samples = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "# Iterate over each unique class value\n",
        "for class_value in range(5):\n",
        "    # Filter the DataFrame to select samples with the current class value\n",
        "    class_samples = df[df['cell_type_idx'] == class_value].sample(n=3, random_state=42)\n",
        "    \n",
        "    # Append the selected samples to the collected_samples DataFrame\n",
        "    collected_samples = pd.concat([collected_samples, class_samples], ignore_index=True)"
      ],
      "metadata": {
        "id": "DdhB8yBeAtOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "images = collected_samples['path']\n",
        "\n",
        "progress_bar = tqdm(total=len(images))\n",
        "\n",
        "for img in images:\n",
        "  img_arr = read_and_resize_image(image_path=img, size=image_size)\n",
        "  im_tensor = PreprocessImages([img_arr])\n",
        "  im = img_arr.astype(np.float32)\n",
        "  vanilla_integrated_gradients_mask_3d = smoothgrad.GetSmoothedMask(\n",
        "    im, call_model_function, call_model_args)\n",
        "  raw_gradients.append(vanilla_integrated_gradients_mask_3d)\n",
        "  \n",
        "  # Update the progress bar\n",
        "  progress_bar.update(1)\n",
        "\n",
        "# Close the progress bar\n",
        "progress_bar.close()"
      ],
      "metadata": {
        "id": "fe1WZKB92AuA",
        "ExecuteTime": {
          "end_time": "2023-05-13T11:34:13.383166700Z",
          "start_time": "2023-05-13T11:33:24.147667800Z"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "segment_image_list = load_images_from_folder(\"/content/content/HAM10000_segmentations_lesion_tschandl\")"
      ],
      "metadata": {
        "id": "hm-D3L80RVoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "boolean_masks = []\n",
        "for img in segment_image_list:\n",
        "    boolean_masks.append(convert_to_boolean_mask(img))"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T12:31:46.207680600Z",
          "start_time": "2023-05-13T12:31:34.524780200Z"
        },
        "id": "yj5_y0_Jc5Gg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "overlap = []\n",
        "for i in range(len(images)):\n",
        "    per = [calculate_overlap(raw_gradients[i][:, :, 0], boolean_masks[i]), np.argmax(predictions, axis=1)[i]]\n",
        "    if str(per[0]) != \"nan\":\n",
        "      overlap.append(per)\n"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T12:33:03.713782Z",
          "start_time": "2023-05-13T12:33:03.671023500Z"
        },
        "id": "DbjKVuGVc5Gg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Calculate average overlap for each class\n",
        "class_overlaps = defaultdict(list)\n",
        "for item in overlap:\n",
        "    overlap_value = item[0]\n",
        "    class_label = item[1]\n",
        "    class_overlaps[class_label].append(overlap_value)\n",
        "\n",
        "class_labels = []\n",
        "average_overlaps = []\n",
        "\n",
        "for class_label, overlaps in class_overlaps.items():\n",
        "    class_labels.append(class_label)\n",
        "    average_overlap = sum(overlaps) / len(overlaps)\n",
        "    average_overlaps.append(average_overlap)\n",
        "\n",
        "# Create a bar chart or scatter plot\n",
        "plt.figure(figsize=(10, 6))  # Adjust the figure size as per your preference\n",
        "\n",
        "mapped_labels = [list(lesion_type_dict.values())[label] for label in class_labels]\n",
        "\n",
        "plt.figure(figsize=(20, 6))\n",
        "# Bar chart\n",
        "plt.bar(range(len(average_overlaps)), average_overlaps)\n",
        "plt.xticks(range(len(average_overlaps)), mapped_labels)\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Average Overlap')\n",
        "plt.title('Average Overlap of Gradients over Masks')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-05-13T12:34:59.508194500Z",
          "start_time": "2023-05-13T12:34:48.572324700Z"
        },
        "id": "biHVKI-Uc5Gg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for label, overlap in zip(mapped_labels, average_overlaps):\n",
        "    print(f\"Label: {label}, Average Overlap: {overlap}\")"
      ],
      "metadata": {
        "id": "09ZgOYyG6VVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "plot_loss(history)"
      ],
      "metadata": {
        "id": "tOVxcIGqc5Gh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP"
      ],
      "metadata": {
        "id": "M_KhKcFZ_G73"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "EffNet = model\n",
        "from keras.layers import Input, concatenate\n",
        "\n",
        "def perceptron():\n",
        "  mlp = Sequential()\n",
        "  mlp.add(Input(shape=(3,)))\n",
        "  mlp.add(Flatten())\n",
        "  mlp.add(Dense(128, activation='relu'))\n",
        "  mlp.add(Dropout(0.5))\n",
        "  mlp.add(Dense(64, activation='relu'))\n",
        "  mlp.add(Dense(len(df['cell_type_idx'].unique()), activation='softmax'))\n",
        "  return mlp"
      ],
      "metadata": {
        "id": "OWw3_1ZypsCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron = perceptron()\n",
        "perceptron.compile(optimizer = 'adam', loss= 'sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "duVgHNta_KxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sex_dict = {\n",
        "    'male': 0.0,\n",
        "    'female': 1.0,\n",
        "    'unknown': 1.5\n",
        "}\n",
        "\n",
        "loc_dict = {\n",
        "    'back': 1.0,\n",
        "    'lower extremity': 2.0,\n",
        "    'trunk': 3.0,\n",
        "    'upper extremity': 4.0,\n",
        "    'abdomen': 5.0,\n",
        "    'face': 6.0,\n",
        "    'chest': 7.0,\n",
        "    'foot': 8.0,\n",
        "    'unknown': 9.0,\n",
        "    'neck': 10.0,\n",
        "    'scalp': 11.0,\n",
        "    'hand': 12.0,\n",
        "    'ear': 13.0,\n",
        "    'genital': 14.0,\n",
        "    'acral': 15.0\n",
        "}\n",
        "\n",
        "train_df = train_df.replace({\"sex\": sex_dict})\n",
        "train_df = train_df.replace({\"localization\": loc_dict})\n",
        "test_df = test_df.replace({\"sex\": sex_dict})\n",
        "test_df = test_df.replace({\"localization\": loc_dict})\n",
        "\n",
        "#Extract and recombine the demographic data for training set and test set\n",
        "X_train_demo = np.stack((np.asarray(train_df['age'].values), np.asarray(train_df['sex'].values), np.asarray(train_df['localization'].values)))\n",
        "y_train = train_df['cell_type_idx'].values\n",
        "X_test_demo = np.stack((np.asarray(test_df['age'].values), np.asarray(test_df['sex'].values), np.asarray(test_df['localization'].values)))\n",
        "y_test = test_df['cell_type_idx'].values\n",
        "\n",
        "X_train_demo=X_train_demo.T.astype(int)\n",
        "X_test_demo=X_test_demo.T.astype(int)"
      ],
      "metadata": {
        "id": "s7mjulC9ASEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron_history = perceptron.fit(X_train_demo, y_train, batch_size=256, epochs=50, validation_split=0.3)"
      ],
      "metadata": {
        "id": "ffiWyIHVAVDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the trained model to make predictions on the test data\n",
        "output_metrics(perceptron, X_test_demo, y_test)\n",
        "plot_confusion_matrix(perceptron, X_test_demo, y_test)\n",
        "plot_loss(perceptron_history)"
      ],
      "metadata": {
        "id": "rrQIs7KeAY7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concatenated_inputs = concatenate(inputs=[perceptron.output, EffNet.output])\n",
        "x = Dense(128, activation=\"relu\")(concatenated_inputs)\n",
        "x = Dense(64, activation=\"relu\")(x)\n",
        "x = Dense(len(df['cell_type_idx'].unique()), activation='softmax')(x)\n",
        "model = Model(inputs=[perceptron.input, EffNet.input], outputs=x)\n",
        "#plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Q5m8FslyAZVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_history = model.fit([X_train_demo, X_train], y_train, batch_size=256, epochs=50, validation_split=0.3)"
      ],
      "metadata": {
        "id": "0IOiBAbEAj4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_metrics(model, [X_test_demo, X_test], y_test)\n",
        "plot_confusion_matrix(model, [X_test_demo, X_test], y_test)\n",
        "plot_loss(model_history)"
      ],
      "metadata": {
        "id": "72yUPebpApIf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}